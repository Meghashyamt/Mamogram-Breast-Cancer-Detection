{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Breast_cancer_detection_cnn1d .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "VKOxZeo2vpM1"
      },
      "source": [
        "# Find the direction of dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzHWc9KyvpND",
        "outputId": "3b6bfa37-7bbf-46fc-8059-deaa084bde29"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Conv1D, MaxPool1D, Flatten\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import AveragePooling1D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import random # for visualization\n",
        "print('Libraries Imported')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Libraries Imported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgUOsfL4v7Am"
      },
      "source": [
        "#!unzip archive.zip"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLC6w_13wra4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a64d41-6a3b-402c-8c8a-14f140e98030"
      },
      "source": [
        "#import pictures from drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmUPjWCmwsSr"
      },
      "source": [
        "#!unzip -uq \"/content/drive/My Drive/MIAS/archive.zip\" -d \"/content/drive/My Drive/MIAS\""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN_-lejyvpNG"
      },
      "source": [
        "path = '/content/drive/My Drive/Data/'"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xnwocimvpNH",
        "outputId": "011feedc-c453-4d54-a459-6a0848b87887"
      },
      "source": [
        "print(\"reading dataframe\")\n",
        "info=pd.read_csv(\"/content/drive/My Drive/Data/data.csv\")\n",
        "info=info.drop('Unnamed: 32',axis=1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading dataframe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "I9QH5n4jvpNI",
        "outputId": "05980d43-d1ba-4d64-eeae-5db396b6231b"
      },
      "source": [
        "info"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>926424</td>\n",
              "      <td>M</td>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>1.1760</td>\n",
              "      <td>1.2560</td>\n",
              "      <td>7.673</td>\n",
              "      <td>158.70</td>\n",
              "      <td>0.010300</td>\n",
              "      <td>0.02891</td>\n",
              "      <td>0.05198</td>\n",
              "      <td>0.02454</td>\n",
              "      <td>0.01114</td>\n",
              "      <td>0.004239</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>926682</td>\n",
              "      <td>M</td>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>0.7655</td>\n",
              "      <td>2.4630</td>\n",
              "      <td>5.203</td>\n",
              "      <td>99.04</td>\n",
              "      <td>0.005769</td>\n",
              "      <td>0.02423</td>\n",
              "      <td>0.03950</td>\n",
              "      <td>0.01678</td>\n",
              "      <td>0.01898</td>\n",
              "      <td>0.002498</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>926954</td>\n",
              "      <td>M</td>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>0.4564</td>\n",
              "      <td>1.0750</td>\n",
              "      <td>3.425</td>\n",
              "      <td>48.55</td>\n",
              "      <td>0.005903</td>\n",
              "      <td>0.03731</td>\n",
              "      <td>0.04730</td>\n",
              "      <td>0.01557</td>\n",
              "      <td>0.01318</td>\n",
              "      <td>0.003892</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>927241</td>\n",
              "      <td>M</td>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>0.7260</td>\n",
              "      <td>1.5950</td>\n",
              "      <td>5.772</td>\n",
              "      <td>86.22</td>\n",
              "      <td>0.006522</td>\n",
              "      <td>0.06158</td>\n",
              "      <td>0.07117</td>\n",
              "      <td>0.01664</td>\n",
              "      <td>0.02324</td>\n",
              "      <td>0.006185</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>92751</td>\n",
              "      <td>B</td>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>1.4280</td>\n",
              "      <td>2.548</td>\n",
              "      <td>19.15</td>\n",
              "      <td>0.007189</td>\n",
              "      <td>0.00466</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.02676</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows Ã— 32 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           id diagnosis  ...  symmetry_worst  fractal_dimension_worst\n",
              "0      842302         M  ...          0.4601                  0.11890\n",
              "1      842517         M  ...          0.2750                  0.08902\n",
              "2    84300903         M  ...          0.3613                  0.08758\n",
              "3    84348301         M  ...          0.6638                  0.17300\n",
              "4    84358402         M  ...          0.2364                  0.07678\n",
              "..        ...       ...  ...             ...                      ...\n",
              "564    926424         M  ...          0.2060                  0.07115\n",
              "565    926682         M  ...          0.2572                  0.06637\n",
              "566    926954         M  ...          0.2218                  0.07820\n",
              "567    927241         M  ...          0.4087                  0.12400\n",
              "568     92751         B  ...          0.2871                  0.07039\n",
              "\n",
              "[569 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDcajV-yrmYo"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "info['diagnosis']=le.fit_transform(info['diagnosis'])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKtCBVoXvpNJ"
      },
      "source": [
        "#info.dropna(subset = [\"SEVERITY\"], inplace=True)\n",
        "\n",
        "#info.reset_index(inplace = True)\n",
        "#info"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9FevP9CvpNJ"
      },
      "source": [
        "#info = info.drop([3], axis=0)\n",
        "#info.reset_index(inplace = True)\n",
        "#info"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9uBZS19Oremi",
        "outputId": "53172995-9b8c-46a9-fe1d-f296621b9174"
      },
      "source": [
        "info.corr()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.039769</td>\n",
              "      <td>0.074626</td>\n",
              "      <td>0.099770</td>\n",
              "      <td>0.073159</td>\n",
              "      <td>0.096893</td>\n",
              "      <td>-0.012968</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.050080</td>\n",
              "      <td>0.044158</td>\n",
              "      <td>-0.022114</td>\n",
              "      <td>-0.052511</td>\n",
              "      <td>0.143048</td>\n",
              "      <td>-0.007526</td>\n",
              "      <td>0.137331</td>\n",
              "      <td>0.177742</td>\n",
              "      <td>0.096781</td>\n",
              "      <td>0.033961</td>\n",
              "      <td>0.055239</td>\n",
              "      <td>0.078768</td>\n",
              "      <td>-0.017306</td>\n",
              "      <td>0.025725</td>\n",
              "      <td>0.082405</td>\n",
              "      <td>0.064720</td>\n",
              "      <td>0.079986</td>\n",
              "      <td>0.107187</td>\n",
              "      <td>0.010338</td>\n",
              "      <td>-0.002968</td>\n",
              "      <td>0.023203</td>\n",
              "      <td>0.035174</td>\n",
              "      <td>-0.044224</td>\n",
              "      <td>-0.029866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>diagnosis</th>\n",
              "      <td>0.039769</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.730029</td>\n",
              "      <td>0.415185</td>\n",
              "      <td>0.742636</td>\n",
              "      <td>0.708984</td>\n",
              "      <td>0.358560</td>\n",
              "      <td>0.596534</td>\n",
              "      <td>0.696360</td>\n",
              "      <td>0.776614</td>\n",
              "      <td>0.330499</td>\n",
              "      <td>-0.012838</td>\n",
              "      <td>0.567134</td>\n",
              "      <td>-0.008303</td>\n",
              "      <td>0.556141</td>\n",
              "      <td>0.548236</td>\n",
              "      <td>-0.067016</td>\n",
              "      <td>0.292999</td>\n",
              "      <td>0.253730</td>\n",
              "      <td>0.408042</td>\n",
              "      <td>-0.006522</td>\n",
              "      <td>0.077972</td>\n",
              "      <td>0.776454</td>\n",
              "      <td>0.456903</td>\n",
              "      <td>0.782914</td>\n",
              "      <td>0.733825</td>\n",
              "      <td>0.421465</td>\n",
              "      <td>0.590998</td>\n",
              "      <td>0.659610</td>\n",
              "      <td>0.793566</td>\n",
              "      <td>0.416294</td>\n",
              "      <td>0.323872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radius_mean</th>\n",
              "      <td>0.074626</td>\n",
              "      <td>0.730029</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.323782</td>\n",
              "      <td>0.997855</td>\n",
              "      <td>0.987357</td>\n",
              "      <td>0.170581</td>\n",
              "      <td>0.506124</td>\n",
              "      <td>0.676764</td>\n",
              "      <td>0.822529</td>\n",
              "      <td>0.147741</td>\n",
              "      <td>-0.311631</td>\n",
              "      <td>0.679090</td>\n",
              "      <td>-0.097317</td>\n",
              "      <td>0.674172</td>\n",
              "      <td>0.735864</td>\n",
              "      <td>-0.222600</td>\n",
              "      <td>0.206000</td>\n",
              "      <td>0.194204</td>\n",
              "      <td>0.376169</td>\n",
              "      <td>-0.104321</td>\n",
              "      <td>-0.042641</td>\n",
              "      <td>0.969539</td>\n",
              "      <td>0.297008</td>\n",
              "      <td>0.965137</td>\n",
              "      <td>0.941082</td>\n",
              "      <td>0.119616</td>\n",
              "      <td>0.413463</td>\n",
              "      <td>0.526911</td>\n",
              "      <td>0.744214</td>\n",
              "      <td>0.163953</td>\n",
              "      <td>0.007066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texture_mean</th>\n",
              "      <td>0.099770</td>\n",
              "      <td>0.415185</td>\n",
              "      <td>0.323782</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.329533</td>\n",
              "      <td>0.321086</td>\n",
              "      <td>-0.023389</td>\n",
              "      <td>0.236702</td>\n",
              "      <td>0.302418</td>\n",
              "      <td>0.293464</td>\n",
              "      <td>0.071401</td>\n",
              "      <td>-0.076437</td>\n",
              "      <td>0.275869</td>\n",
              "      <td>0.386358</td>\n",
              "      <td>0.281673</td>\n",
              "      <td>0.259845</td>\n",
              "      <td>0.006614</td>\n",
              "      <td>0.191975</td>\n",
              "      <td>0.143293</td>\n",
              "      <td>0.163851</td>\n",
              "      <td>0.009127</td>\n",
              "      <td>0.054458</td>\n",
              "      <td>0.352573</td>\n",
              "      <td>0.912045</td>\n",
              "      <td>0.358040</td>\n",
              "      <td>0.343546</td>\n",
              "      <td>0.077503</td>\n",
              "      <td>0.277830</td>\n",
              "      <td>0.301025</td>\n",
              "      <td>0.295316</td>\n",
              "      <td>0.105008</td>\n",
              "      <td>0.119205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perimeter_mean</th>\n",
              "      <td>0.073159</td>\n",
              "      <td>0.742636</td>\n",
              "      <td>0.997855</td>\n",
              "      <td>0.329533</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.986507</td>\n",
              "      <td>0.207278</td>\n",
              "      <td>0.556936</td>\n",
              "      <td>0.716136</td>\n",
              "      <td>0.850977</td>\n",
              "      <td>0.183027</td>\n",
              "      <td>-0.261477</td>\n",
              "      <td>0.691765</td>\n",
              "      <td>-0.086761</td>\n",
              "      <td>0.693135</td>\n",
              "      <td>0.744983</td>\n",
              "      <td>-0.202694</td>\n",
              "      <td>0.250744</td>\n",
              "      <td>0.228082</td>\n",
              "      <td>0.407217</td>\n",
              "      <td>-0.081629</td>\n",
              "      <td>-0.005523</td>\n",
              "      <td>0.969476</td>\n",
              "      <td>0.303038</td>\n",
              "      <td>0.970387</td>\n",
              "      <td>0.941550</td>\n",
              "      <td>0.150549</td>\n",
              "      <td>0.455774</td>\n",
              "      <td>0.563879</td>\n",
              "      <td>0.771241</td>\n",
              "      <td>0.189115</td>\n",
              "      <td>0.051019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>area_mean</th>\n",
              "      <td>0.096893</td>\n",
              "      <td>0.708984</td>\n",
              "      <td>0.987357</td>\n",
              "      <td>0.321086</td>\n",
              "      <td>0.986507</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.177028</td>\n",
              "      <td>0.498502</td>\n",
              "      <td>0.685983</td>\n",
              "      <td>0.823269</td>\n",
              "      <td>0.151293</td>\n",
              "      <td>-0.283110</td>\n",
              "      <td>0.732562</td>\n",
              "      <td>-0.066280</td>\n",
              "      <td>0.726628</td>\n",
              "      <td>0.800086</td>\n",
              "      <td>-0.166777</td>\n",
              "      <td>0.212583</td>\n",
              "      <td>0.207660</td>\n",
              "      <td>0.372320</td>\n",
              "      <td>-0.072497</td>\n",
              "      <td>-0.019887</td>\n",
              "      <td>0.962746</td>\n",
              "      <td>0.287489</td>\n",
              "      <td>0.959120</td>\n",
              "      <td>0.959213</td>\n",
              "      <td>0.123523</td>\n",
              "      <td>0.390410</td>\n",
              "      <td>0.512606</td>\n",
              "      <td>0.722017</td>\n",
              "      <td>0.143570</td>\n",
              "      <td>0.003738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smoothness_mean</th>\n",
              "      <td>-0.012968</td>\n",
              "      <td>0.358560</td>\n",
              "      <td>0.170581</td>\n",
              "      <td>-0.023389</td>\n",
              "      <td>0.207278</td>\n",
              "      <td>0.177028</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.659123</td>\n",
              "      <td>0.521984</td>\n",
              "      <td>0.553695</td>\n",
              "      <td>0.557775</td>\n",
              "      <td>0.584792</td>\n",
              "      <td>0.301467</td>\n",
              "      <td>0.068406</td>\n",
              "      <td>0.296092</td>\n",
              "      <td>0.246552</td>\n",
              "      <td>0.332375</td>\n",
              "      <td>0.318943</td>\n",
              "      <td>0.248396</td>\n",
              "      <td>0.380676</td>\n",
              "      <td>0.200774</td>\n",
              "      <td>0.283607</td>\n",
              "      <td>0.213120</td>\n",
              "      <td>0.036072</td>\n",
              "      <td>0.238853</td>\n",
              "      <td>0.206718</td>\n",
              "      <td>0.805324</td>\n",
              "      <td>0.472468</td>\n",
              "      <td>0.434926</td>\n",
              "      <td>0.503053</td>\n",
              "      <td>0.394309</td>\n",
              "      <td>0.499316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compactness_mean</th>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.596534</td>\n",
              "      <td>0.506124</td>\n",
              "      <td>0.236702</td>\n",
              "      <td>0.556936</td>\n",
              "      <td>0.498502</td>\n",
              "      <td>0.659123</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.883121</td>\n",
              "      <td>0.831135</td>\n",
              "      <td>0.602641</td>\n",
              "      <td>0.565369</td>\n",
              "      <td>0.497473</td>\n",
              "      <td>0.046205</td>\n",
              "      <td>0.548905</td>\n",
              "      <td>0.455653</td>\n",
              "      <td>0.135299</td>\n",
              "      <td>0.738722</td>\n",
              "      <td>0.570517</td>\n",
              "      <td>0.642262</td>\n",
              "      <td>0.229977</td>\n",
              "      <td>0.507318</td>\n",
              "      <td>0.535315</td>\n",
              "      <td>0.248133</td>\n",
              "      <td>0.590210</td>\n",
              "      <td>0.509604</td>\n",
              "      <td>0.565541</td>\n",
              "      <td>0.865809</td>\n",
              "      <td>0.816275</td>\n",
              "      <td>0.815573</td>\n",
              "      <td>0.510223</td>\n",
              "      <td>0.687382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concavity_mean</th>\n",
              "      <td>0.050080</td>\n",
              "      <td>0.696360</td>\n",
              "      <td>0.676764</td>\n",
              "      <td>0.302418</td>\n",
              "      <td>0.716136</td>\n",
              "      <td>0.685983</td>\n",
              "      <td>0.521984</td>\n",
              "      <td>0.883121</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.921391</td>\n",
              "      <td>0.500667</td>\n",
              "      <td>0.336783</td>\n",
              "      <td>0.631925</td>\n",
              "      <td>0.076218</td>\n",
              "      <td>0.660391</td>\n",
              "      <td>0.617427</td>\n",
              "      <td>0.098564</td>\n",
              "      <td>0.670279</td>\n",
              "      <td>0.691270</td>\n",
              "      <td>0.683260</td>\n",
              "      <td>0.178009</td>\n",
              "      <td>0.449301</td>\n",
              "      <td>0.688236</td>\n",
              "      <td>0.299879</td>\n",
              "      <td>0.729565</td>\n",
              "      <td>0.675987</td>\n",
              "      <td>0.448822</td>\n",
              "      <td>0.754968</td>\n",
              "      <td>0.884103</td>\n",
              "      <td>0.861323</td>\n",
              "      <td>0.409464</td>\n",
              "      <td>0.514930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concave points_mean</th>\n",
              "      <td>0.044158</td>\n",
              "      <td>0.776614</td>\n",
              "      <td>0.822529</td>\n",
              "      <td>0.293464</td>\n",
              "      <td>0.850977</td>\n",
              "      <td>0.823269</td>\n",
              "      <td>0.553695</td>\n",
              "      <td>0.831135</td>\n",
              "      <td>0.921391</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.462497</td>\n",
              "      <td>0.166917</td>\n",
              "      <td>0.698050</td>\n",
              "      <td>0.021480</td>\n",
              "      <td>0.710650</td>\n",
              "      <td>0.690299</td>\n",
              "      <td>0.027653</td>\n",
              "      <td>0.490424</td>\n",
              "      <td>0.439167</td>\n",
              "      <td>0.615634</td>\n",
              "      <td>0.095351</td>\n",
              "      <td>0.257584</td>\n",
              "      <td>0.830318</td>\n",
              "      <td>0.292752</td>\n",
              "      <td>0.855923</td>\n",
              "      <td>0.809630</td>\n",
              "      <td>0.452753</td>\n",
              "      <td>0.667454</td>\n",
              "      <td>0.752399</td>\n",
              "      <td>0.910155</td>\n",
              "      <td>0.375744</td>\n",
              "      <td>0.368661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>symmetry_mean</th>\n",
              "      <td>-0.022114</td>\n",
              "      <td>0.330499</td>\n",
              "      <td>0.147741</td>\n",
              "      <td>0.071401</td>\n",
              "      <td>0.183027</td>\n",
              "      <td>0.151293</td>\n",
              "      <td>0.557775</td>\n",
              "      <td>0.602641</td>\n",
              "      <td>0.500667</td>\n",
              "      <td>0.462497</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.479921</td>\n",
              "      <td>0.303379</td>\n",
              "      <td>0.128053</td>\n",
              "      <td>0.313893</td>\n",
              "      <td>0.223970</td>\n",
              "      <td>0.187321</td>\n",
              "      <td>0.421659</td>\n",
              "      <td>0.342627</td>\n",
              "      <td>0.393298</td>\n",
              "      <td>0.449137</td>\n",
              "      <td>0.331786</td>\n",
              "      <td>0.185728</td>\n",
              "      <td>0.090651</td>\n",
              "      <td>0.219169</td>\n",
              "      <td>0.177193</td>\n",
              "      <td>0.426675</td>\n",
              "      <td>0.473200</td>\n",
              "      <td>0.433721</td>\n",
              "      <td>0.430297</td>\n",
              "      <td>0.699826</td>\n",
              "      <td>0.438413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <td>-0.052511</td>\n",
              "      <td>-0.012838</td>\n",
              "      <td>-0.311631</td>\n",
              "      <td>-0.076437</td>\n",
              "      <td>-0.261477</td>\n",
              "      <td>-0.283110</td>\n",
              "      <td>0.584792</td>\n",
              "      <td>0.565369</td>\n",
              "      <td>0.336783</td>\n",
              "      <td>0.166917</td>\n",
              "      <td>0.479921</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.164174</td>\n",
              "      <td>0.039830</td>\n",
              "      <td>-0.090170</td>\n",
              "      <td>0.401964</td>\n",
              "      <td>0.559837</td>\n",
              "      <td>0.446630</td>\n",
              "      <td>0.341198</td>\n",
              "      <td>0.345007</td>\n",
              "      <td>0.688132</td>\n",
              "      <td>-0.253691</td>\n",
              "      <td>-0.051269</td>\n",
              "      <td>-0.205151</td>\n",
              "      <td>-0.231854</td>\n",
              "      <td>0.504942</td>\n",
              "      <td>0.458798</td>\n",
              "      <td>0.346234</td>\n",
              "      <td>0.175325</td>\n",
              "      <td>0.334019</td>\n",
              "      <td>0.767297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radius_se</th>\n",
              "      <td>0.143048</td>\n",
              "      <td>0.567134</td>\n",
              "      <td>0.679090</td>\n",
              "      <td>0.275869</td>\n",
              "      <td>0.691765</td>\n",
              "      <td>0.732562</td>\n",
              "      <td>0.301467</td>\n",
              "      <td>0.497473</td>\n",
              "      <td>0.631925</td>\n",
              "      <td>0.698050</td>\n",
              "      <td>0.303379</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.213247</td>\n",
              "      <td>0.972794</td>\n",
              "      <td>0.951830</td>\n",
              "      <td>0.164514</td>\n",
              "      <td>0.356065</td>\n",
              "      <td>0.332358</td>\n",
              "      <td>0.513346</td>\n",
              "      <td>0.240567</td>\n",
              "      <td>0.227754</td>\n",
              "      <td>0.715065</td>\n",
              "      <td>0.194799</td>\n",
              "      <td>0.719684</td>\n",
              "      <td>0.751548</td>\n",
              "      <td>0.141919</td>\n",
              "      <td>0.287103</td>\n",
              "      <td>0.380585</td>\n",
              "      <td>0.531062</td>\n",
              "      <td>0.094543</td>\n",
              "      <td>0.049559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texture_se</th>\n",
              "      <td>-0.007526</td>\n",
              "      <td>-0.008303</td>\n",
              "      <td>-0.097317</td>\n",
              "      <td>0.386358</td>\n",
              "      <td>-0.086761</td>\n",
              "      <td>-0.066280</td>\n",
              "      <td>0.068406</td>\n",
              "      <td>0.046205</td>\n",
              "      <td>0.076218</td>\n",
              "      <td>0.021480</td>\n",
              "      <td>0.128053</td>\n",
              "      <td>0.164174</td>\n",
              "      <td>0.213247</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.223171</td>\n",
              "      <td>0.111567</td>\n",
              "      <td>0.397243</td>\n",
              "      <td>0.231700</td>\n",
              "      <td>0.194998</td>\n",
              "      <td>0.230283</td>\n",
              "      <td>0.411621</td>\n",
              "      <td>0.279723</td>\n",
              "      <td>-0.111690</td>\n",
              "      <td>0.409003</td>\n",
              "      <td>-0.102242</td>\n",
              "      <td>-0.083195</td>\n",
              "      <td>-0.073658</td>\n",
              "      <td>-0.092439</td>\n",
              "      <td>-0.068956</td>\n",
              "      <td>-0.119638</td>\n",
              "      <td>-0.128215</td>\n",
              "      <td>-0.045655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perimeter_se</th>\n",
              "      <td>0.137331</td>\n",
              "      <td>0.556141</td>\n",
              "      <td>0.674172</td>\n",
              "      <td>0.281673</td>\n",
              "      <td>0.693135</td>\n",
              "      <td>0.726628</td>\n",
              "      <td>0.296092</td>\n",
              "      <td>0.548905</td>\n",
              "      <td>0.660391</td>\n",
              "      <td>0.710650</td>\n",
              "      <td>0.313893</td>\n",
              "      <td>0.039830</td>\n",
              "      <td>0.972794</td>\n",
              "      <td>0.223171</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.937655</td>\n",
              "      <td>0.151075</td>\n",
              "      <td>0.416322</td>\n",
              "      <td>0.362482</td>\n",
              "      <td>0.556264</td>\n",
              "      <td>0.266487</td>\n",
              "      <td>0.244143</td>\n",
              "      <td>0.697201</td>\n",
              "      <td>0.200371</td>\n",
              "      <td>0.721031</td>\n",
              "      <td>0.730713</td>\n",
              "      <td>0.130054</td>\n",
              "      <td>0.341919</td>\n",
              "      <td>0.418899</td>\n",
              "      <td>0.554897</td>\n",
              "      <td>0.109930</td>\n",
              "      <td>0.085433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>area_se</th>\n",
              "      <td>0.177742</td>\n",
              "      <td>0.548236</td>\n",
              "      <td>0.735864</td>\n",
              "      <td>0.259845</td>\n",
              "      <td>0.744983</td>\n",
              "      <td>0.800086</td>\n",
              "      <td>0.246552</td>\n",
              "      <td>0.455653</td>\n",
              "      <td>0.617427</td>\n",
              "      <td>0.690299</td>\n",
              "      <td>0.223970</td>\n",
              "      <td>-0.090170</td>\n",
              "      <td>0.951830</td>\n",
              "      <td>0.111567</td>\n",
              "      <td>0.937655</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.075150</td>\n",
              "      <td>0.284840</td>\n",
              "      <td>0.270895</td>\n",
              "      <td>0.415730</td>\n",
              "      <td>0.134109</td>\n",
              "      <td>0.127071</td>\n",
              "      <td>0.757373</td>\n",
              "      <td>0.196497</td>\n",
              "      <td>0.761213</td>\n",
              "      <td>0.811408</td>\n",
              "      <td>0.125389</td>\n",
              "      <td>0.283257</td>\n",
              "      <td>0.385100</td>\n",
              "      <td>0.538166</td>\n",
              "      <td>0.074126</td>\n",
              "      <td>0.017539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smoothness_se</th>\n",
              "      <td>0.096781</td>\n",
              "      <td>-0.067016</td>\n",
              "      <td>-0.222600</td>\n",
              "      <td>0.006614</td>\n",
              "      <td>-0.202694</td>\n",
              "      <td>-0.166777</td>\n",
              "      <td>0.332375</td>\n",
              "      <td>0.135299</td>\n",
              "      <td>0.098564</td>\n",
              "      <td>0.027653</td>\n",
              "      <td>0.187321</td>\n",
              "      <td>0.401964</td>\n",
              "      <td>0.164514</td>\n",
              "      <td>0.397243</td>\n",
              "      <td>0.151075</td>\n",
              "      <td>0.075150</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.336696</td>\n",
              "      <td>0.268685</td>\n",
              "      <td>0.328429</td>\n",
              "      <td>0.413506</td>\n",
              "      <td>0.427374</td>\n",
              "      <td>-0.230691</td>\n",
              "      <td>-0.074743</td>\n",
              "      <td>-0.217304</td>\n",
              "      <td>-0.182195</td>\n",
              "      <td>0.314457</td>\n",
              "      <td>-0.055558</td>\n",
              "      <td>-0.058298</td>\n",
              "      <td>-0.102007</td>\n",
              "      <td>-0.107342</td>\n",
              "      <td>0.101480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compactness_se</th>\n",
              "      <td>0.033961</td>\n",
              "      <td>0.292999</td>\n",
              "      <td>0.206000</td>\n",
              "      <td>0.191975</td>\n",
              "      <td>0.250744</td>\n",
              "      <td>0.212583</td>\n",
              "      <td>0.318943</td>\n",
              "      <td>0.738722</td>\n",
              "      <td>0.670279</td>\n",
              "      <td>0.490424</td>\n",
              "      <td>0.421659</td>\n",
              "      <td>0.559837</td>\n",
              "      <td>0.356065</td>\n",
              "      <td>0.231700</td>\n",
              "      <td>0.416322</td>\n",
              "      <td>0.284840</td>\n",
              "      <td>0.336696</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.801268</td>\n",
              "      <td>0.744083</td>\n",
              "      <td>0.394713</td>\n",
              "      <td>0.803269</td>\n",
              "      <td>0.204607</td>\n",
              "      <td>0.143003</td>\n",
              "      <td>0.260516</td>\n",
              "      <td>0.199371</td>\n",
              "      <td>0.227394</td>\n",
              "      <td>0.678780</td>\n",
              "      <td>0.639147</td>\n",
              "      <td>0.483208</td>\n",
              "      <td>0.277878</td>\n",
              "      <td>0.590973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concavity_se</th>\n",
              "      <td>0.055239</td>\n",
              "      <td>0.253730</td>\n",
              "      <td>0.194204</td>\n",
              "      <td>0.143293</td>\n",
              "      <td>0.228082</td>\n",
              "      <td>0.207660</td>\n",
              "      <td>0.248396</td>\n",
              "      <td>0.570517</td>\n",
              "      <td>0.691270</td>\n",
              "      <td>0.439167</td>\n",
              "      <td>0.342627</td>\n",
              "      <td>0.446630</td>\n",
              "      <td>0.332358</td>\n",
              "      <td>0.194998</td>\n",
              "      <td>0.362482</td>\n",
              "      <td>0.270895</td>\n",
              "      <td>0.268685</td>\n",
              "      <td>0.801268</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.771804</td>\n",
              "      <td>0.309429</td>\n",
              "      <td>0.727372</td>\n",
              "      <td>0.186904</td>\n",
              "      <td>0.100241</td>\n",
              "      <td>0.226680</td>\n",
              "      <td>0.188353</td>\n",
              "      <td>0.168481</td>\n",
              "      <td>0.484858</td>\n",
              "      <td>0.662564</td>\n",
              "      <td>0.440472</td>\n",
              "      <td>0.197788</td>\n",
              "      <td>0.439329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concave points_se</th>\n",
              "      <td>0.078768</td>\n",
              "      <td>0.408042</td>\n",
              "      <td>0.376169</td>\n",
              "      <td>0.163851</td>\n",
              "      <td>0.407217</td>\n",
              "      <td>0.372320</td>\n",
              "      <td>0.380676</td>\n",
              "      <td>0.642262</td>\n",
              "      <td>0.683260</td>\n",
              "      <td>0.615634</td>\n",
              "      <td>0.393298</td>\n",
              "      <td>0.341198</td>\n",
              "      <td>0.513346</td>\n",
              "      <td>0.230283</td>\n",
              "      <td>0.556264</td>\n",
              "      <td>0.415730</td>\n",
              "      <td>0.328429</td>\n",
              "      <td>0.744083</td>\n",
              "      <td>0.771804</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.312780</td>\n",
              "      <td>0.611044</td>\n",
              "      <td>0.358127</td>\n",
              "      <td>0.086741</td>\n",
              "      <td>0.394999</td>\n",
              "      <td>0.342271</td>\n",
              "      <td>0.215351</td>\n",
              "      <td>0.452888</td>\n",
              "      <td>0.549592</td>\n",
              "      <td>0.602450</td>\n",
              "      <td>0.143116</td>\n",
              "      <td>0.310655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>symmetry_se</th>\n",
              "      <td>-0.017306</td>\n",
              "      <td>-0.006522</td>\n",
              "      <td>-0.104321</td>\n",
              "      <td>0.009127</td>\n",
              "      <td>-0.081629</td>\n",
              "      <td>-0.072497</td>\n",
              "      <td>0.200774</td>\n",
              "      <td>0.229977</td>\n",
              "      <td>0.178009</td>\n",
              "      <td>0.095351</td>\n",
              "      <td>0.449137</td>\n",
              "      <td>0.345007</td>\n",
              "      <td>0.240567</td>\n",
              "      <td>0.411621</td>\n",
              "      <td>0.266487</td>\n",
              "      <td>0.134109</td>\n",
              "      <td>0.413506</td>\n",
              "      <td>0.394713</td>\n",
              "      <td>0.309429</td>\n",
              "      <td>0.312780</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.369078</td>\n",
              "      <td>-0.128121</td>\n",
              "      <td>-0.077473</td>\n",
              "      <td>-0.103753</td>\n",
              "      <td>-0.110343</td>\n",
              "      <td>-0.012662</td>\n",
              "      <td>0.060255</td>\n",
              "      <td>0.037119</td>\n",
              "      <td>-0.030413</td>\n",
              "      <td>0.389402</td>\n",
              "      <td>0.078079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <td>0.025725</td>\n",
              "      <td>0.077972</td>\n",
              "      <td>-0.042641</td>\n",
              "      <td>0.054458</td>\n",
              "      <td>-0.005523</td>\n",
              "      <td>-0.019887</td>\n",
              "      <td>0.283607</td>\n",
              "      <td>0.507318</td>\n",
              "      <td>0.449301</td>\n",
              "      <td>0.257584</td>\n",
              "      <td>0.331786</td>\n",
              "      <td>0.688132</td>\n",
              "      <td>0.227754</td>\n",
              "      <td>0.279723</td>\n",
              "      <td>0.244143</td>\n",
              "      <td>0.127071</td>\n",
              "      <td>0.427374</td>\n",
              "      <td>0.803269</td>\n",
              "      <td>0.727372</td>\n",
              "      <td>0.611044</td>\n",
              "      <td>0.369078</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.037488</td>\n",
              "      <td>-0.003195</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>-0.022736</td>\n",
              "      <td>0.170568</td>\n",
              "      <td>0.390159</td>\n",
              "      <td>0.379975</td>\n",
              "      <td>0.215204</td>\n",
              "      <td>0.111094</td>\n",
              "      <td>0.591328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radius_worst</th>\n",
              "      <td>0.082405</td>\n",
              "      <td>0.776454</td>\n",
              "      <td>0.969539</td>\n",
              "      <td>0.352573</td>\n",
              "      <td>0.969476</td>\n",
              "      <td>0.962746</td>\n",
              "      <td>0.213120</td>\n",
              "      <td>0.535315</td>\n",
              "      <td>0.688236</td>\n",
              "      <td>0.830318</td>\n",
              "      <td>0.185728</td>\n",
              "      <td>-0.253691</td>\n",
              "      <td>0.715065</td>\n",
              "      <td>-0.111690</td>\n",
              "      <td>0.697201</td>\n",
              "      <td>0.757373</td>\n",
              "      <td>-0.230691</td>\n",
              "      <td>0.204607</td>\n",
              "      <td>0.186904</td>\n",
              "      <td>0.358127</td>\n",
              "      <td>-0.128121</td>\n",
              "      <td>-0.037488</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.359921</td>\n",
              "      <td>0.993708</td>\n",
              "      <td>0.984015</td>\n",
              "      <td>0.216574</td>\n",
              "      <td>0.475820</td>\n",
              "      <td>0.573975</td>\n",
              "      <td>0.787424</td>\n",
              "      <td>0.243529</td>\n",
              "      <td>0.093492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texture_worst</th>\n",
              "      <td>0.064720</td>\n",
              "      <td>0.456903</td>\n",
              "      <td>0.297008</td>\n",
              "      <td>0.912045</td>\n",
              "      <td>0.303038</td>\n",
              "      <td>0.287489</td>\n",
              "      <td>0.036072</td>\n",
              "      <td>0.248133</td>\n",
              "      <td>0.299879</td>\n",
              "      <td>0.292752</td>\n",
              "      <td>0.090651</td>\n",
              "      <td>-0.051269</td>\n",
              "      <td>0.194799</td>\n",
              "      <td>0.409003</td>\n",
              "      <td>0.200371</td>\n",
              "      <td>0.196497</td>\n",
              "      <td>-0.074743</td>\n",
              "      <td>0.143003</td>\n",
              "      <td>0.100241</td>\n",
              "      <td>0.086741</td>\n",
              "      <td>-0.077473</td>\n",
              "      <td>-0.003195</td>\n",
              "      <td>0.359921</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.365098</td>\n",
              "      <td>0.345842</td>\n",
              "      <td>0.225429</td>\n",
              "      <td>0.360832</td>\n",
              "      <td>0.368366</td>\n",
              "      <td>0.359755</td>\n",
              "      <td>0.233027</td>\n",
              "      <td>0.219122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perimeter_worst</th>\n",
              "      <td>0.079986</td>\n",
              "      <td>0.782914</td>\n",
              "      <td>0.965137</td>\n",
              "      <td>0.358040</td>\n",
              "      <td>0.970387</td>\n",
              "      <td>0.959120</td>\n",
              "      <td>0.238853</td>\n",
              "      <td>0.590210</td>\n",
              "      <td>0.729565</td>\n",
              "      <td>0.855923</td>\n",
              "      <td>0.219169</td>\n",
              "      <td>-0.205151</td>\n",
              "      <td>0.719684</td>\n",
              "      <td>-0.102242</td>\n",
              "      <td>0.721031</td>\n",
              "      <td>0.761213</td>\n",
              "      <td>-0.217304</td>\n",
              "      <td>0.260516</td>\n",
              "      <td>0.226680</td>\n",
              "      <td>0.394999</td>\n",
              "      <td>-0.103753</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>0.993708</td>\n",
              "      <td>0.365098</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.977578</td>\n",
              "      <td>0.236775</td>\n",
              "      <td>0.529408</td>\n",
              "      <td>0.618344</td>\n",
              "      <td>0.816322</td>\n",
              "      <td>0.269493</td>\n",
              "      <td>0.138957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>area_worst</th>\n",
              "      <td>0.107187</td>\n",
              "      <td>0.733825</td>\n",
              "      <td>0.941082</td>\n",
              "      <td>0.343546</td>\n",
              "      <td>0.941550</td>\n",
              "      <td>0.959213</td>\n",
              "      <td>0.206718</td>\n",
              "      <td>0.509604</td>\n",
              "      <td>0.675987</td>\n",
              "      <td>0.809630</td>\n",
              "      <td>0.177193</td>\n",
              "      <td>-0.231854</td>\n",
              "      <td>0.751548</td>\n",
              "      <td>-0.083195</td>\n",
              "      <td>0.730713</td>\n",
              "      <td>0.811408</td>\n",
              "      <td>-0.182195</td>\n",
              "      <td>0.199371</td>\n",
              "      <td>0.188353</td>\n",
              "      <td>0.342271</td>\n",
              "      <td>-0.110343</td>\n",
              "      <td>-0.022736</td>\n",
              "      <td>0.984015</td>\n",
              "      <td>0.345842</td>\n",
              "      <td>0.977578</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.209145</td>\n",
              "      <td>0.438296</td>\n",
              "      <td>0.543331</td>\n",
              "      <td>0.747419</td>\n",
              "      <td>0.209146</td>\n",
              "      <td>0.079647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smoothness_worst</th>\n",
              "      <td>0.010338</td>\n",
              "      <td>0.421465</td>\n",
              "      <td>0.119616</td>\n",
              "      <td>0.077503</td>\n",
              "      <td>0.150549</td>\n",
              "      <td>0.123523</td>\n",
              "      <td>0.805324</td>\n",
              "      <td>0.565541</td>\n",
              "      <td>0.448822</td>\n",
              "      <td>0.452753</td>\n",
              "      <td>0.426675</td>\n",
              "      <td>0.504942</td>\n",
              "      <td>0.141919</td>\n",
              "      <td>-0.073658</td>\n",
              "      <td>0.130054</td>\n",
              "      <td>0.125389</td>\n",
              "      <td>0.314457</td>\n",
              "      <td>0.227394</td>\n",
              "      <td>0.168481</td>\n",
              "      <td>0.215351</td>\n",
              "      <td>-0.012662</td>\n",
              "      <td>0.170568</td>\n",
              "      <td>0.216574</td>\n",
              "      <td>0.225429</td>\n",
              "      <td>0.236775</td>\n",
              "      <td>0.209145</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.568187</td>\n",
              "      <td>0.518523</td>\n",
              "      <td>0.547691</td>\n",
              "      <td>0.493838</td>\n",
              "      <td>0.617624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compactness_worst</th>\n",
              "      <td>-0.002968</td>\n",
              "      <td>0.590998</td>\n",
              "      <td>0.413463</td>\n",
              "      <td>0.277830</td>\n",
              "      <td>0.455774</td>\n",
              "      <td>0.390410</td>\n",
              "      <td>0.472468</td>\n",
              "      <td>0.865809</td>\n",
              "      <td>0.754968</td>\n",
              "      <td>0.667454</td>\n",
              "      <td>0.473200</td>\n",
              "      <td>0.458798</td>\n",
              "      <td>0.287103</td>\n",
              "      <td>-0.092439</td>\n",
              "      <td>0.341919</td>\n",
              "      <td>0.283257</td>\n",
              "      <td>-0.055558</td>\n",
              "      <td>0.678780</td>\n",
              "      <td>0.484858</td>\n",
              "      <td>0.452888</td>\n",
              "      <td>0.060255</td>\n",
              "      <td>0.390159</td>\n",
              "      <td>0.475820</td>\n",
              "      <td>0.360832</td>\n",
              "      <td>0.529408</td>\n",
              "      <td>0.438296</td>\n",
              "      <td>0.568187</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.892261</td>\n",
              "      <td>0.801080</td>\n",
              "      <td>0.614441</td>\n",
              "      <td>0.810455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concavity_worst</th>\n",
              "      <td>0.023203</td>\n",
              "      <td>0.659610</td>\n",
              "      <td>0.526911</td>\n",
              "      <td>0.301025</td>\n",
              "      <td>0.563879</td>\n",
              "      <td>0.512606</td>\n",
              "      <td>0.434926</td>\n",
              "      <td>0.816275</td>\n",
              "      <td>0.884103</td>\n",
              "      <td>0.752399</td>\n",
              "      <td>0.433721</td>\n",
              "      <td>0.346234</td>\n",
              "      <td>0.380585</td>\n",
              "      <td>-0.068956</td>\n",
              "      <td>0.418899</td>\n",
              "      <td>0.385100</td>\n",
              "      <td>-0.058298</td>\n",
              "      <td>0.639147</td>\n",
              "      <td>0.662564</td>\n",
              "      <td>0.549592</td>\n",
              "      <td>0.037119</td>\n",
              "      <td>0.379975</td>\n",
              "      <td>0.573975</td>\n",
              "      <td>0.368366</td>\n",
              "      <td>0.618344</td>\n",
              "      <td>0.543331</td>\n",
              "      <td>0.518523</td>\n",
              "      <td>0.892261</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.855434</td>\n",
              "      <td>0.532520</td>\n",
              "      <td>0.686511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concave points_worst</th>\n",
              "      <td>0.035174</td>\n",
              "      <td>0.793566</td>\n",
              "      <td>0.744214</td>\n",
              "      <td>0.295316</td>\n",
              "      <td>0.771241</td>\n",
              "      <td>0.722017</td>\n",
              "      <td>0.503053</td>\n",
              "      <td>0.815573</td>\n",
              "      <td>0.861323</td>\n",
              "      <td>0.910155</td>\n",
              "      <td>0.430297</td>\n",
              "      <td>0.175325</td>\n",
              "      <td>0.531062</td>\n",
              "      <td>-0.119638</td>\n",
              "      <td>0.554897</td>\n",
              "      <td>0.538166</td>\n",
              "      <td>-0.102007</td>\n",
              "      <td>0.483208</td>\n",
              "      <td>0.440472</td>\n",
              "      <td>0.602450</td>\n",
              "      <td>-0.030413</td>\n",
              "      <td>0.215204</td>\n",
              "      <td>0.787424</td>\n",
              "      <td>0.359755</td>\n",
              "      <td>0.816322</td>\n",
              "      <td>0.747419</td>\n",
              "      <td>0.547691</td>\n",
              "      <td>0.801080</td>\n",
              "      <td>0.855434</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.502528</td>\n",
              "      <td>0.511114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>symmetry_worst</th>\n",
              "      <td>-0.044224</td>\n",
              "      <td>0.416294</td>\n",
              "      <td>0.163953</td>\n",
              "      <td>0.105008</td>\n",
              "      <td>0.189115</td>\n",
              "      <td>0.143570</td>\n",
              "      <td>0.394309</td>\n",
              "      <td>0.510223</td>\n",
              "      <td>0.409464</td>\n",
              "      <td>0.375744</td>\n",
              "      <td>0.699826</td>\n",
              "      <td>0.334019</td>\n",
              "      <td>0.094543</td>\n",
              "      <td>-0.128215</td>\n",
              "      <td>0.109930</td>\n",
              "      <td>0.074126</td>\n",
              "      <td>-0.107342</td>\n",
              "      <td>0.277878</td>\n",
              "      <td>0.197788</td>\n",
              "      <td>0.143116</td>\n",
              "      <td>0.389402</td>\n",
              "      <td>0.111094</td>\n",
              "      <td>0.243529</td>\n",
              "      <td>0.233027</td>\n",
              "      <td>0.269493</td>\n",
              "      <td>0.209146</td>\n",
              "      <td>0.493838</td>\n",
              "      <td>0.614441</td>\n",
              "      <td>0.532520</td>\n",
              "      <td>0.502528</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.537848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <td>-0.029866</td>\n",
              "      <td>0.323872</td>\n",
              "      <td>0.007066</td>\n",
              "      <td>0.119205</td>\n",
              "      <td>0.051019</td>\n",
              "      <td>0.003738</td>\n",
              "      <td>0.499316</td>\n",
              "      <td>0.687382</td>\n",
              "      <td>0.514930</td>\n",
              "      <td>0.368661</td>\n",
              "      <td>0.438413</td>\n",
              "      <td>0.767297</td>\n",
              "      <td>0.049559</td>\n",
              "      <td>-0.045655</td>\n",
              "      <td>0.085433</td>\n",
              "      <td>0.017539</td>\n",
              "      <td>0.101480</td>\n",
              "      <td>0.590973</td>\n",
              "      <td>0.439329</td>\n",
              "      <td>0.310655</td>\n",
              "      <td>0.078079</td>\n",
              "      <td>0.591328</td>\n",
              "      <td>0.093492</td>\n",
              "      <td>0.219122</td>\n",
              "      <td>0.138957</td>\n",
              "      <td>0.079647</td>\n",
              "      <td>0.617624</td>\n",
              "      <td>0.810455</td>\n",
              "      <td>0.686511</td>\n",
              "      <td>0.511114</td>\n",
              "      <td>0.537848</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               id  ...  fractal_dimension_worst\n",
              "id                       1.000000  ...                -0.029866\n",
              "diagnosis                0.039769  ...                 0.323872\n",
              "radius_mean              0.074626  ...                 0.007066\n",
              "texture_mean             0.099770  ...                 0.119205\n",
              "perimeter_mean           0.073159  ...                 0.051019\n",
              "area_mean                0.096893  ...                 0.003738\n",
              "smoothness_mean         -0.012968  ...                 0.499316\n",
              "compactness_mean         0.000096  ...                 0.687382\n",
              "concavity_mean           0.050080  ...                 0.514930\n",
              "concave points_mean      0.044158  ...                 0.368661\n",
              "symmetry_mean           -0.022114  ...                 0.438413\n",
              "fractal_dimension_mean  -0.052511  ...                 0.767297\n",
              "radius_se                0.143048  ...                 0.049559\n",
              "texture_se              -0.007526  ...                -0.045655\n",
              "perimeter_se             0.137331  ...                 0.085433\n",
              "area_se                  0.177742  ...                 0.017539\n",
              "smoothness_se            0.096781  ...                 0.101480\n",
              "compactness_se           0.033961  ...                 0.590973\n",
              "concavity_se             0.055239  ...                 0.439329\n",
              "concave points_se        0.078768  ...                 0.310655\n",
              "symmetry_se             -0.017306  ...                 0.078079\n",
              "fractal_dimension_se     0.025725  ...                 0.591328\n",
              "radius_worst             0.082405  ...                 0.093492\n",
              "texture_worst            0.064720  ...                 0.219122\n",
              "perimeter_worst          0.079986  ...                 0.138957\n",
              "area_worst               0.107187  ...                 0.079647\n",
              "smoothness_worst         0.010338  ...                 0.617624\n",
              "compactness_worst       -0.002968  ...                 0.810455\n",
              "concavity_worst          0.023203  ...                 0.686511\n",
              "concave points_worst     0.035174  ...                 0.511114\n",
              "symmetry_worst          -0.044224  ...                 0.537848\n",
              "fractal_dimension_worst -0.029866  ...                 1.000000\n",
              "\n",
              "[32 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvPqVOG9tiln"
      },
      "source": [
        "info.drop(['fractal_dimension_mean','texture_se','smoothness_se','symmetry_se','fractal_dimension_se'],axis=1,inplace=True)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XYMb5lrvpNR"
      },
      "source": [
        "#last_label = np.array(last_label)\n",
        "#img_path = np.array(img_path)\n",
        "X=info.iloc[:,2:]\n",
        "y=info.diagnosis"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "mn4nB1mIagbU",
        "outputId": "b9d041e1-fff4-4980-b29f-0bc50a23aa96"
      },
      "source": [
        "X"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>1.1760</td>\n",
              "      <td>7.673</td>\n",
              "      <td>158.70</td>\n",
              "      <td>0.02891</td>\n",
              "      <td>0.05198</td>\n",
              "      <td>0.02454</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.7655</td>\n",
              "      <td>5.203</td>\n",
              "      <td>99.04</td>\n",
              "      <td>0.02423</td>\n",
              "      <td>0.03950</td>\n",
              "      <td>0.01678</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.4564</td>\n",
              "      <td>3.425</td>\n",
              "      <td>48.55</td>\n",
              "      <td>0.03731</td>\n",
              "      <td>0.04730</td>\n",
              "      <td>0.01557</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.7260</td>\n",
              "      <td>5.772</td>\n",
              "      <td>86.22</td>\n",
              "      <td>0.06158</td>\n",
              "      <td>0.07117</td>\n",
              "      <td>0.01664</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>2.548</td>\n",
              "      <td>19.15</td>\n",
              "      <td>0.00466</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows Ã— 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     radius_mean  texture_mean  ...  symmetry_worst  fractal_dimension_worst\n",
              "0          17.99         10.38  ...          0.4601                  0.11890\n",
              "1          20.57         17.77  ...          0.2750                  0.08902\n",
              "2          19.69         21.25  ...          0.3613                  0.08758\n",
              "3          11.42         20.38  ...          0.6638                  0.17300\n",
              "4          20.29         14.34  ...          0.2364                  0.07678\n",
              "..           ...           ...  ...             ...                      ...\n",
              "564        21.56         22.39  ...          0.2060                  0.07115\n",
              "565        20.13         28.25  ...          0.2572                  0.06637\n",
              "566        16.60         28.08  ...          0.2218                  0.07820\n",
              "567        20.60         29.33  ...          0.4087                  0.12400\n",
              "568         7.76         24.54  ...          0.2871                  0.07039\n",
              "\n",
              "[569 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L19yrjkRaguI",
        "outputId": "596c36ef-75b4-4ab1-c590-92b60b074888"
      },
      "source": [
        "y"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      1\n",
              "1      1\n",
              "2      1\n",
              "3      1\n",
              "4      1\n",
              "      ..\n",
              "564    1\n",
              "565    1\n",
              "566    1\n",
              "567    1\n",
              "568    0\n",
              "Name: diagnosis, Length: 569, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoJ1U7I2vpNS"
      },
      "source": [
        "# split train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 1)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQez7UtUvpNS",
        "outputId": "7d1925f9-8f48-4241-af6f-cc5707523696"
      },
      "source": [
        "len(X_train),len(X_test),len(y_train),len(y_test)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(455, 114, 455, 114)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5uKoIVFvpNT"
      },
      "source": [
        "#x_train = np.array(x_train)\n",
        "#x_test = np.array(x_test)\n",
        "X_tr = np.array(X_train).reshape(X_train.shape[0], X_train.shape[1], 1).astype('float32')\n",
        "X_vd = np.array(X_test).reshape(X_test.shape[0], X_test.shape[1], 1).astype('float32')\n",
        "\n",
        "y_tr = np.array(y_train)\n",
        "y_vd = np.array(y_test)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lvE0XUKvpNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcce9082-a305-4435-d59b-d6263988237d"
      },
      "source": [
        "X_tr.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(455, 25, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvlaq8J3vpNU"
      },
      "source": [
        "#(a,b,c)=x_train.shape # (35136, 224, 224)\n",
        "#x_train = np.reshape(x_train, (a, b, c, 1)) # 1 for gray scale\n",
        "#(a, b, c)=x_test.shape\n",
        "#x_test = np.reshape(x_test, (a, b, c, 1))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81VwpAT1vpNV"
      },
      "source": [
        "\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(32, 5, input_shape = X_tr.shape[1:3]))\n",
        "    #model.add(Conv2D(64, kernel_size=(3,3),activation='relu'))\n",
        "    #model.add(MaxPool1D(pool_size=2))\n",
        "\n",
        "    #model.add(Conv1D(64, kernel_size=3,activation='relu'))\n",
        "    #model.add(MaxPool1D(pool_size=2))\n",
        "    #model.add(Dropout(0.25))\n",
        "  \n",
        "    #model.add(Dense(64, activation='relu'))\n",
        "    #model.add(Dropout(0.25))\n",
        "    #model.add(Flatten())\n",
        "    model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(Conv1D(64, 1))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, input_dim=25))\n",
        "    model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(Dense(32))\n",
        "    model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtWB9ANFvpNW",
        "outputId": "053c9818-759d-4870-bc7b-11c7381eedd7"
      },
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 21, 32)            192       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 21, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 21, 64)            2112      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1344)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               172160    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 178,625\n",
            "Trainable params: 178,625\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LAZbS6Jt6hI"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 1)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCMygyxYt6R6"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(10, input_dim=25, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLmInV-Dt9Qo",
        "outputId": "749415f8-2fdd-45b3-82cc-69243508feff"
      },
      "source": [
        "m=model.fit(X_train,y_train, epochs=300, batch_size=10, validation_split=0.2)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "37/37 [==============================] - 1s 7ms/step - loss: 53.4253 - accuracy: 0.3744 - val_loss: 29.1979 - val_accuracy: 0.3956\n",
            "Epoch 2/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 27.1792 - accuracy: 0.3499 - val_loss: 14.0860 - val_accuracy: 0.3956\n",
            "Epoch 3/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 11.9854 - accuracy: 0.3720 - val_loss: 5.1030 - val_accuracy: 0.3956\n",
            "Epoch 4/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 3.8023 - accuracy: 0.3690 - val_loss: 0.7273 - val_accuracy: 0.6813\n",
            "Epoch 5/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6392 - accuracy: 0.7343 - val_loss: 0.4515 - val_accuracy: 0.8901\n",
            "Epoch 6/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.4864 - accuracy: 0.8746 - val_loss: 0.4390 - val_accuracy: 0.8901\n",
            "Epoch 7/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.4600 - accuracy: 0.8534 - val_loss: 0.4265 - val_accuracy: 0.8901\n",
            "Epoch 8/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.4382 - accuracy: 0.8601 - val_loss: 0.4136 - val_accuracy: 0.8901\n",
            "Epoch 9/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4348 - accuracy: 0.8943 - val_loss: 0.4008 - val_accuracy: 0.8901\n",
            "Epoch 10/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4462 - accuracy: 0.8497 - val_loss: 0.3917 - val_accuracy: 0.8791\n",
            "Epoch 11/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4161 - accuracy: 0.8739 - val_loss: 0.3760 - val_accuracy: 0.8901\n",
            "Epoch 12/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4188 - accuracy: 0.8659 - val_loss: 0.3635 - val_accuracy: 0.8901\n",
            "Epoch 13/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3933 - accuracy: 0.8702 - val_loss: 0.3557 - val_accuracy: 0.8901\n",
            "Epoch 14/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3526 - accuracy: 0.8959 - val_loss: 0.3423 - val_accuracy: 0.8901\n",
            "Epoch 15/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.3659 - accuracy: 0.8660 - val_loss: 0.3331 - val_accuracy: 0.8901\n",
            "Epoch 16/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3580 - accuracy: 0.8713 - val_loss: 0.3263 - val_accuracy: 0.8901\n",
            "Epoch 17/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3287 - accuracy: 0.8841 - val_loss: 0.3186 - val_accuracy: 0.8901\n",
            "Epoch 18/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3671 - accuracy: 0.8610 - val_loss: 0.3105 - val_accuracy: 0.8901\n",
            "Epoch 19/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3271 - accuracy: 0.8844 - val_loss: 0.3059 - val_accuracy: 0.8901\n",
            "Epoch 20/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3272 - accuracy: 0.8897 - val_loss: 0.3015 - val_accuracy: 0.8901\n",
            "Epoch 21/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3423 - accuracy: 0.8630 - val_loss: 0.3004 - val_accuracy: 0.9011\n",
            "Epoch 22/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3194 - accuracy: 0.8967 - val_loss: 0.2929 - val_accuracy: 0.8901\n",
            "Epoch 23/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.8716 - val_loss: 0.2900 - val_accuracy: 0.8901\n",
            "Epoch 24/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2835 - accuracy: 0.9082 - val_loss: 0.2872 - val_accuracy: 0.8901\n",
            "Epoch 25/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3127 - accuracy: 0.8903 - val_loss: 0.2847 - val_accuracy: 0.8901\n",
            "Epoch 26/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.3344 - accuracy: 0.8767 - val_loss: 0.2801 - val_accuracy: 0.8901\n",
            "Epoch 27/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3381 - accuracy: 0.8645 - val_loss: 0.2673 - val_accuracy: 0.8901\n",
            "Epoch 28/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3235 - accuracy: 0.8818 - val_loss: 0.2773 - val_accuracy: 0.8901\n",
            "Epoch 29/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2644 - accuracy: 0.9174 - val_loss: 0.2672 - val_accuracy: 0.8901\n",
            "Epoch 30/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3076 - accuracy: 0.8723 - val_loss: 0.2762 - val_accuracy: 0.8901\n",
            "Epoch 31/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2718 - accuracy: 0.9040 - val_loss: 0.2554 - val_accuracy: 0.9011\n",
            "Epoch 32/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2860 - accuracy: 0.9023 - val_loss: 0.2531 - val_accuracy: 0.9011\n",
            "Epoch 33/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2936 - accuracy: 0.9011 - val_loss: 0.2537 - val_accuracy: 0.9121\n",
            "Epoch 34/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.2934 - accuracy: 0.8824 - val_loss: 0.2469 - val_accuracy: 0.9011\n",
            "Epoch 35/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3144 - accuracy: 0.8843 - val_loss: 0.2590 - val_accuracy: 0.9121\n",
            "Epoch 36/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2914 - accuracy: 0.8886 - val_loss: 0.2608 - val_accuracy: 0.8901\n",
            "Epoch 37/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2989 - accuracy: 0.8881 - val_loss: 0.2413 - val_accuracy: 0.8901\n",
            "Epoch 38/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3524 - accuracy: 0.8526 - val_loss: 0.2400 - val_accuracy: 0.8901\n",
            "Epoch 39/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2948 - accuracy: 0.8903 - val_loss: 0.2406 - val_accuracy: 0.9011\n",
            "Epoch 40/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2722 - accuracy: 0.8960 - val_loss: 0.2397 - val_accuracy: 0.8901\n",
            "Epoch 41/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2995 - accuracy: 0.8865 - val_loss: 0.2339 - val_accuracy: 0.9011\n",
            "Epoch 42/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2804 - accuracy: 0.8779 - val_loss: 0.2247 - val_accuracy: 0.9011\n",
            "Epoch 43/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2574 - accuracy: 0.9001 - val_loss: 0.2070 - val_accuracy: 0.9011\n",
            "Epoch 44/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2123 - accuracy: 0.9217 - val_loss: 0.2092 - val_accuracy: 0.8901\n",
            "Epoch 45/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2149 - accuracy: 0.9069 - val_loss: 0.2130 - val_accuracy: 0.9011\n",
            "Epoch 46/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2304 - accuracy: 0.9019 - val_loss: 0.2016 - val_accuracy: 0.8901\n",
            "Epoch 47/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2401 - accuracy: 0.8853 - val_loss: 0.1911 - val_accuracy: 0.9011\n",
            "Epoch 48/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2355 - accuracy: 0.8818 - val_loss: 0.1990 - val_accuracy: 0.9011\n",
            "Epoch 49/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2310 - accuracy: 0.9105 - val_loss: 0.2243 - val_accuracy: 0.9341\n",
            "Epoch 50/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3110 - accuracy: 0.8697 - val_loss: 0.2190 - val_accuracy: 0.9121\n",
            "Epoch 51/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2093 - accuracy: 0.9188 - val_loss: 0.2143 - val_accuracy: 0.8901\n",
            "Epoch 52/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2632 - accuracy: 0.8805 - val_loss: 0.1894 - val_accuracy: 0.9121\n",
            "Epoch 53/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2259 - accuracy: 0.9138 - val_loss: 0.1839 - val_accuracy: 0.9231\n",
            "Epoch 54/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2444 - accuracy: 0.8913 - val_loss: 0.1891 - val_accuracy: 0.9121\n",
            "Epoch 55/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1952 - accuracy: 0.9163 - val_loss: 0.1907 - val_accuracy: 0.9121\n",
            "Epoch 56/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2368 - accuracy: 0.9104 - val_loss: 0.2140 - val_accuracy: 0.9231\n",
            "Epoch 57/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2492 - accuracy: 0.8784 - val_loss: 0.1761 - val_accuracy: 0.9231\n",
            "Epoch 58/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1797 - accuracy: 0.9173 - val_loss: 0.1905 - val_accuracy: 0.9341\n",
            "Epoch 59/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.2396 - accuracy: 0.9043 - val_loss: 0.2049 - val_accuracy: 0.9121\n",
            "Epoch 60/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2304 - accuracy: 0.9056 - val_loss: 0.1878 - val_accuracy: 0.9011\n",
            "Epoch 61/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2556 - accuracy: 0.8927 - val_loss: 0.1896 - val_accuracy: 0.8901\n",
            "Epoch 62/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2468 - accuracy: 0.8779 - val_loss: 0.2086 - val_accuracy: 0.9231\n",
            "Epoch 63/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2208 - accuracy: 0.9057 - val_loss: 0.1816 - val_accuracy: 0.8901\n",
            "Epoch 64/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2584 - accuracy: 0.8904 - val_loss: 0.1679 - val_accuracy: 0.9011\n",
            "Epoch 65/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2239 - accuracy: 0.9105 - val_loss: 0.1867 - val_accuracy: 0.9011\n",
            "Epoch 66/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.2509 - accuracy: 0.8885 - val_loss: 0.1819 - val_accuracy: 0.9341\n",
            "Epoch 67/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2452 - accuracy: 0.8916 - val_loss: 0.1956 - val_accuracy: 0.9011\n",
            "Epoch 68/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2344 - accuracy: 0.8951 - val_loss: 0.1838 - val_accuracy: 0.9231\n",
            "Epoch 69/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2276 - accuracy: 0.9119 - val_loss: 0.1757 - val_accuracy: 0.8901\n",
            "Epoch 70/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2307 - accuracy: 0.8887 - val_loss: 0.1804 - val_accuracy: 0.9011\n",
            "Epoch 71/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2394 - accuracy: 0.8972 - val_loss: 0.1890 - val_accuracy: 0.9341\n",
            "Epoch 72/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2114 - accuracy: 0.9085 - val_loss: 0.1665 - val_accuracy: 0.9231\n",
            "Epoch 73/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2068 - accuracy: 0.9160 - val_loss: 0.1688 - val_accuracy: 0.9231\n",
            "Epoch 74/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2160 - accuracy: 0.9012 - val_loss: 0.1834 - val_accuracy: 0.9231\n",
            "Epoch 75/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2027 - accuracy: 0.9248 - val_loss: 0.1759 - val_accuracy: 0.8901\n",
            "Epoch 76/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1855 - accuracy: 0.9145 - val_loss: 0.1902 - val_accuracy: 0.9341\n",
            "Epoch 77/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2191 - accuracy: 0.9119 - val_loss: 0.1583 - val_accuracy: 0.9341\n",
            "Epoch 78/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2360 - accuracy: 0.8804 - val_loss: 0.1722 - val_accuracy: 0.9011\n",
            "Epoch 79/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2283 - accuracy: 0.8945 - val_loss: 0.1646 - val_accuracy: 0.9341\n",
            "Epoch 80/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.2048 - accuracy: 0.9043 - val_loss: 0.1794 - val_accuracy: 0.9011\n",
            "Epoch 81/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1850 - accuracy: 0.9285 - val_loss: 0.1599 - val_accuracy: 0.9341\n",
            "Epoch 82/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1923 - accuracy: 0.9286 - val_loss: 0.1740 - val_accuracy: 0.8901\n",
            "Epoch 83/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1825 - accuracy: 0.9133 - val_loss: 0.1712 - val_accuracy: 0.9341\n",
            "Epoch 84/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1683 - accuracy: 0.9331 - val_loss: 0.1968 - val_accuracy: 0.9231\n",
            "Epoch 85/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.2295 - accuracy: 0.8982 - val_loss: 0.1663 - val_accuracy: 0.8901\n",
            "Epoch 86/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2202 - accuracy: 0.8939 - val_loss: 0.1755 - val_accuracy: 0.9011\n",
            "Epoch 87/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2217 - accuracy: 0.8966 - val_loss: 0.1590 - val_accuracy: 0.9231\n",
            "Epoch 88/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1833 - accuracy: 0.9116 - val_loss: 0.1812 - val_accuracy: 0.9231\n",
            "Epoch 89/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.2273 - accuracy: 0.9018 - val_loss: 0.1595 - val_accuracy: 0.9231\n",
            "Epoch 90/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2290 - accuracy: 0.9034 - val_loss: 0.1731 - val_accuracy: 0.8901\n",
            "Epoch 91/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1798 - accuracy: 0.9205 - val_loss: 0.1711 - val_accuracy: 0.9341\n",
            "Epoch 92/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1727 - accuracy: 0.9424 - val_loss: 0.1615 - val_accuracy: 0.9341\n",
            "Epoch 93/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2287 - accuracy: 0.8953 - val_loss: 0.1584 - val_accuracy: 0.9341\n",
            "Epoch 94/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2142 - accuracy: 0.9077 - val_loss: 0.1780 - val_accuracy: 0.9341\n",
            "Epoch 95/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2136 - accuracy: 0.9158 - val_loss: 0.1614 - val_accuracy: 0.9121\n",
            "Epoch 96/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1945 - accuracy: 0.9071 - val_loss: 0.1898 - val_accuracy: 0.9341\n",
            "Epoch 97/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2141 - accuracy: 0.9093 - val_loss: 0.1563 - val_accuracy: 0.9341\n",
            "Epoch 98/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1795 - accuracy: 0.9259 - val_loss: 0.1710 - val_accuracy: 0.9341\n",
            "Epoch 99/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1811 - accuracy: 0.9301 - val_loss: 0.1676 - val_accuracy: 0.9231\n",
            "Epoch 100/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1936 - accuracy: 0.9068 - val_loss: 0.1538 - val_accuracy: 0.9341\n",
            "Epoch 101/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1729 - accuracy: 0.9293 - val_loss: 0.1819 - val_accuracy: 0.9341\n",
            "Epoch 102/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2113 - accuracy: 0.9223 - val_loss: 0.1774 - val_accuracy: 0.9341\n",
            "Epoch 103/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2195 - accuracy: 0.8991 - val_loss: 0.1801 - val_accuracy: 0.9341\n",
            "Epoch 104/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1946 - accuracy: 0.9293 - val_loss: 0.1533 - val_accuracy: 0.9121\n",
            "Epoch 105/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1938 - accuracy: 0.9221 - val_loss: 0.1748 - val_accuracy: 0.9341\n",
            "Epoch 106/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2477 - accuracy: 0.8665 - val_loss: 0.1545 - val_accuracy: 0.9231\n",
            "Epoch 107/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1923 - accuracy: 0.9235 - val_loss: 0.1583 - val_accuracy: 0.9011\n",
            "Epoch 108/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2689 - accuracy: 0.8809 - val_loss: 0.1576 - val_accuracy: 0.9341\n",
            "Epoch 109/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2212 - accuracy: 0.8998 - val_loss: 0.1577 - val_accuracy: 0.9341\n",
            "Epoch 110/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1683 - accuracy: 0.9263 - val_loss: 0.1801 - val_accuracy: 0.9341\n",
            "Epoch 111/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1979 - accuracy: 0.9118 - val_loss: 0.1574 - val_accuracy: 0.9231\n",
            "Epoch 112/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1726 - accuracy: 0.9336 - val_loss: 0.1580 - val_accuracy: 0.9341\n",
            "Epoch 113/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1923 - accuracy: 0.9130 - val_loss: 0.1771 - val_accuracy: 0.9121\n",
            "Epoch 114/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2246 - accuracy: 0.8939 - val_loss: 0.1591 - val_accuracy: 0.9451\n",
            "Epoch 115/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1773 - accuracy: 0.9252 - val_loss: 0.1801 - val_accuracy: 0.9451\n",
            "Epoch 116/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1703 - accuracy: 0.9320 - val_loss: 0.1598 - val_accuracy: 0.9231\n",
            "Epoch 117/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1932 - accuracy: 0.9110 - val_loss: 0.1573 - val_accuracy: 0.9341\n",
            "Epoch 118/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1652 - accuracy: 0.9369 - val_loss: 0.1566 - val_accuracy: 0.9231\n",
            "Epoch 119/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1532 - accuracy: 0.9320 - val_loss: 0.1583 - val_accuracy: 0.9451\n",
            "Epoch 120/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1759 - accuracy: 0.9234 - val_loss: 0.1578 - val_accuracy: 0.9231\n",
            "Epoch 121/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2480 - accuracy: 0.8928 - val_loss: 0.1569 - val_accuracy: 0.9341\n",
            "Epoch 122/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1946 - accuracy: 0.9080 - val_loss: 0.1682 - val_accuracy: 0.9341\n",
            "Epoch 123/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1925 - accuracy: 0.9268 - val_loss: 0.1588 - val_accuracy: 0.9451\n",
            "Epoch 124/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1735 - accuracy: 0.9154 - val_loss: 0.1614 - val_accuracy: 0.9341\n",
            "Epoch 125/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1921 - accuracy: 0.9109 - val_loss: 0.1592 - val_accuracy: 0.9231\n",
            "Epoch 126/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1697 - accuracy: 0.9324 - val_loss: 0.1575 - val_accuracy: 0.9341\n",
            "Epoch 127/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.2393 - accuracy: 0.9051 - val_loss: 0.1568 - val_accuracy: 0.9341\n",
            "Epoch 128/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2433 - accuracy: 0.8898 - val_loss: 0.1614 - val_accuracy: 0.9451\n",
            "Epoch 129/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1900 - accuracy: 0.9190 - val_loss: 0.1555 - val_accuracy: 0.9451\n",
            "Epoch 130/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1947 - accuracy: 0.9223 - val_loss: 0.1580 - val_accuracy: 0.9231\n",
            "Epoch 131/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1751 - accuracy: 0.9292 - val_loss: 0.1793 - val_accuracy: 0.9231\n",
            "Epoch 132/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1962 - accuracy: 0.9298 - val_loss: 0.1555 - val_accuracy: 0.9341\n",
            "Epoch 133/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1778 - accuracy: 0.9143 - val_loss: 0.1566 - val_accuracy: 0.9451\n",
            "Epoch 134/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1943 - accuracy: 0.9246 - val_loss: 0.1569 - val_accuracy: 0.9341\n",
            "Epoch 135/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1633 - accuracy: 0.9269 - val_loss: 0.1620 - val_accuracy: 0.9341\n",
            "Epoch 136/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1678 - accuracy: 0.9312 - val_loss: 0.1577 - val_accuracy: 0.9231\n",
            "Epoch 137/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1694 - accuracy: 0.9313 - val_loss: 0.1603 - val_accuracy: 0.9341\n",
            "Epoch 138/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1682 - accuracy: 0.9288 - val_loss: 0.1650 - val_accuracy: 0.9341\n",
            "Epoch 139/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2130 - accuracy: 0.9013 - val_loss: 0.1634 - val_accuracy: 0.9341\n",
            "Epoch 140/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1671 - accuracy: 0.9240 - val_loss: 0.1619 - val_accuracy: 0.9451\n",
            "Epoch 141/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1725 - accuracy: 0.9299 - val_loss: 0.1554 - val_accuracy: 0.9451\n",
            "Epoch 142/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1868 - accuracy: 0.9124 - val_loss: 0.1598 - val_accuracy: 0.9341\n",
            "Epoch 143/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.9051 - val_loss: 0.1705 - val_accuracy: 0.9341\n",
            "Epoch 144/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1869 - accuracy: 0.9239 - val_loss: 0.1542 - val_accuracy: 0.9451\n",
            "Epoch 145/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2231 - accuracy: 0.8944 - val_loss: 0.1578 - val_accuracy: 0.9341\n",
            "Epoch 146/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1619 - accuracy: 0.9331 - val_loss: 0.1731 - val_accuracy: 0.9451\n",
            "Epoch 147/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1926 - accuracy: 0.9167 - val_loss: 0.1618 - val_accuracy: 0.9231\n",
            "Epoch 148/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2001 - accuracy: 0.9121 - val_loss: 0.1600 - val_accuracy: 0.9341\n",
            "Epoch 149/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1947 - accuracy: 0.9202 - val_loss: 0.1557 - val_accuracy: 0.9341\n",
            "Epoch 150/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1866 - accuracy: 0.9316 - val_loss: 0.1543 - val_accuracy: 0.9231\n",
            "Epoch 151/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1988 - accuracy: 0.9165 - val_loss: 0.1563 - val_accuracy: 0.9451\n",
            "Epoch 152/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1862 - accuracy: 0.9083 - val_loss: 0.1589 - val_accuracy: 0.9451\n",
            "Epoch 153/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.2280 - accuracy: 0.8884 - val_loss: 0.1511 - val_accuracy: 0.9341\n",
            "Epoch 154/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1852 - accuracy: 0.9161 - val_loss: 0.1670 - val_accuracy: 0.9341\n",
            "Epoch 155/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1839 - accuracy: 0.9155 - val_loss: 0.1511 - val_accuracy: 0.9121\n",
            "Epoch 156/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1968 - accuracy: 0.9135 - val_loss: 0.1488 - val_accuracy: 0.9451\n",
            "Epoch 157/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2170 - accuracy: 0.9167 - val_loss: 0.1608 - val_accuracy: 0.9341\n",
            "Epoch 158/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1551 - accuracy: 0.9369 - val_loss: 0.1593 - val_accuracy: 0.9121\n",
            "Epoch 159/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1660 - accuracy: 0.9252 - val_loss: 0.1491 - val_accuracy: 0.9341\n",
            "Epoch 160/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1869 - accuracy: 0.9286 - val_loss: 0.1683 - val_accuracy: 0.9231\n",
            "Epoch 161/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2016 - accuracy: 0.9029 - val_loss: 0.1541 - val_accuracy: 0.9451\n",
            "Epoch 162/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1477 - accuracy: 0.9378 - val_loss: 0.1609 - val_accuracy: 0.9451\n",
            "Epoch 163/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1455 - accuracy: 0.9331 - val_loss: 0.1579 - val_accuracy: 0.9231\n",
            "Epoch 164/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2125 - accuracy: 0.9146 - val_loss: 0.1541 - val_accuracy: 0.9341\n",
            "Epoch 165/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1751 - accuracy: 0.9192 - val_loss: 0.1632 - val_accuracy: 0.9451\n",
            "Epoch 166/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1791 - accuracy: 0.9337 - val_loss: 0.1564 - val_accuracy: 0.9451\n",
            "Epoch 167/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1681 - accuracy: 0.9241 - val_loss: 0.1558 - val_accuracy: 0.9341\n",
            "Epoch 168/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1853 - accuracy: 0.9275 - val_loss: 0.1552 - val_accuracy: 0.9121\n",
            "Epoch 169/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1648 - accuracy: 0.9188 - val_loss: 0.1627 - val_accuracy: 0.9451\n",
            "Epoch 170/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1626 - accuracy: 0.9320 - val_loss: 0.1642 - val_accuracy: 0.9451\n",
            "Epoch 171/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1662 - accuracy: 0.9268 - val_loss: 0.1605 - val_accuracy: 0.9231\n",
            "Epoch 172/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1982 - accuracy: 0.9048 - val_loss: 0.1486 - val_accuracy: 0.9451\n",
            "Epoch 173/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1571 - accuracy: 0.9346 - val_loss: 0.1642 - val_accuracy: 0.9341\n",
            "Epoch 174/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1685 - accuracy: 0.9130 - val_loss: 0.1694 - val_accuracy: 0.9121\n",
            "Epoch 175/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1868 - accuracy: 0.9253 - val_loss: 0.1557 - val_accuracy: 0.9121\n",
            "Epoch 176/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1683 - accuracy: 0.9294 - val_loss: 0.1544 - val_accuracy: 0.9231\n",
            "Epoch 177/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1528 - accuracy: 0.9244 - val_loss: 0.1553 - val_accuracy: 0.9341\n",
            "Epoch 178/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1569 - accuracy: 0.9326 - val_loss: 0.1609 - val_accuracy: 0.9451\n",
            "Epoch 179/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1599 - accuracy: 0.9328 - val_loss: 0.1498 - val_accuracy: 0.9451\n",
            "Epoch 180/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1434 - accuracy: 0.9421 - val_loss: 0.1762 - val_accuracy: 0.9451\n",
            "Epoch 181/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1656 - accuracy: 0.9389 - val_loss: 0.1558 - val_accuracy: 0.9231\n",
            "Epoch 182/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.2403 - accuracy: 0.8927 - val_loss: 0.1513 - val_accuracy: 0.9451\n",
            "Epoch 183/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1592 - accuracy: 0.9329 - val_loss: 0.1679 - val_accuracy: 0.9121\n",
            "Epoch 184/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1972 - accuracy: 0.8968 - val_loss: 0.1542 - val_accuracy: 0.9451\n",
            "Epoch 185/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1866 - accuracy: 0.9170 - val_loss: 0.1564 - val_accuracy: 0.9451\n",
            "Epoch 186/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1723 - accuracy: 0.9135 - val_loss: 0.1536 - val_accuracy: 0.9451\n",
            "Epoch 187/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1269 - accuracy: 0.9427 - val_loss: 0.1820 - val_accuracy: 0.9451\n",
            "Epoch 188/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1796 - accuracy: 0.9111 - val_loss: 0.1524 - val_accuracy: 0.9451\n",
            "Epoch 189/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1711 - accuracy: 0.9178 - val_loss: 0.1560 - val_accuracy: 0.9121\n",
            "Epoch 190/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1280 - accuracy: 0.9454 - val_loss: 0.1790 - val_accuracy: 0.9451\n",
            "Epoch 191/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1913 - accuracy: 0.9101 - val_loss: 0.1575 - val_accuracy: 0.9451\n",
            "Epoch 192/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1512 - accuracy: 0.9392 - val_loss: 0.1680 - val_accuracy: 0.9341\n",
            "Epoch 193/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1686 - accuracy: 0.9152 - val_loss: 0.1801 - val_accuracy: 0.9451\n",
            "Epoch 194/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1790 - accuracy: 0.9254 - val_loss: 0.1601 - val_accuracy: 0.9451\n",
            "Epoch 195/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1756 - accuracy: 0.9215 - val_loss: 0.1628 - val_accuracy: 0.9451\n",
            "Epoch 196/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1635 - accuracy: 0.9275 - val_loss: 0.1646 - val_accuracy: 0.9451\n",
            "Epoch 197/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1752 - accuracy: 0.9205 - val_loss: 0.1514 - val_accuracy: 0.9121\n",
            "Epoch 198/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1903 - accuracy: 0.9158 - val_loss: 0.1533 - val_accuracy: 0.9451\n",
            "Epoch 199/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1965 - accuracy: 0.9240 - val_loss: 0.1536 - val_accuracy: 0.9451\n",
            "Epoch 200/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1521 - accuracy: 0.9351 - val_loss: 0.1645 - val_accuracy: 0.9121\n",
            "Epoch 201/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1590 - accuracy: 0.9212 - val_loss: 0.1578 - val_accuracy: 0.9121\n",
            "Epoch 202/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1443 - accuracy: 0.9308 - val_loss: 0.1544 - val_accuracy: 0.9451\n",
            "Epoch 203/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1696 - accuracy: 0.9265 - val_loss: 0.1495 - val_accuracy: 0.9451\n",
            "Epoch 204/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1635 - accuracy: 0.9429 - val_loss: 0.1656 - val_accuracy: 0.9121\n",
            "Epoch 205/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1850 - accuracy: 0.9067 - val_loss: 0.1459 - val_accuracy: 0.9451\n",
            "Epoch 206/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.2168 - accuracy: 0.9014 - val_loss: 0.1540 - val_accuracy: 0.9231\n",
            "Epoch 207/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1724 - accuracy: 0.9204 - val_loss: 0.1487 - val_accuracy: 0.9451\n",
            "Epoch 208/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1703 - accuracy: 0.9130 - val_loss: 0.1495 - val_accuracy: 0.9451\n",
            "Epoch 209/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1924 - accuracy: 0.9096 - val_loss: 0.1735 - val_accuracy: 0.9451\n",
            "Epoch 210/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1546 - accuracy: 0.9393 - val_loss: 0.1751 - val_accuracy: 0.9451\n",
            "Epoch 211/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1675 - accuracy: 0.9284 - val_loss: 0.1609 - val_accuracy: 0.9451\n",
            "Epoch 212/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1471 - accuracy: 0.9322 - val_loss: 0.1731 - val_accuracy: 0.9341\n",
            "Epoch 213/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1573 - accuracy: 0.9361 - val_loss: 0.1659 - val_accuracy: 0.9451\n",
            "Epoch 214/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1809 - accuracy: 0.9224 - val_loss: 0.1573 - val_accuracy: 0.9341\n",
            "Epoch 215/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.2194 - accuracy: 0.8930 - val_loss: 0.1591 - val_accuracy: 0.9341\n",
            "Epoch 216/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.2224 - accuracy: 0.9270 - val_loss: 0.1687 - val_accuracy: 0.9451\n",
            "Epoch 217/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1587 - accuracy: 0.9219 - val_loss: 0.1827 - val_accuracy: 0.9451\n",
            "Epoch 218/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1576 - accuracy: 0.9301 - val_loss: 0.1583 - val_accuracy: 0.9451\n",
            "Epoch 219/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1420 - accuracy: 0.9314 - val_loss: 0.1766 - val_accuracy: 0.9451\n",
            "Epoch 220/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1969 - accuracy: 0.9032 - val_loss: 0.1551 - val_accuracy: 0.9451\n",
            "Epoch 221/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1837 - accuracy: 0.9139 - val_loss: 0.1538 - val_accuracy: 0.9451\n",
            "Epoch 222/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1609 - accuracy: 0.9280 - val_loss: 0.1515 - val_accuracy: 0.9341\n",
            "Epoch 223/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1443 - accuracy: 0.9406 - val_loss: 0.1721 - val_accuracy: 0.9451\n",
            "Epoch 224/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1609 - accuracy: 0.9261 - val_loss: 0.1606 - val_accuracy: 0.9121\n",
            "Epoch 225/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1836 - accuracy: 0.9218 - val_loss: 0.1579 - val_accuracy: 0.9451\n",
            "Epoch 226/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1371 - accuracy: 0.9446 - val_loss: 0.1841 - val_accuracy: 0.9341\n",
            "Epoch 227/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1854 - accuracy: 0.9181 - val_loss: 0.1657 - val_accuracy: 0.9451\n",
            "Epoch 228/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1596 - accuracy: 0.9399 - val_loss: 0.1778 - val_accuracy: 0.9121\n",
            "Epoch 229/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2073 - accuracy: 0.8993 - val_loss: 0.1717 - val_accuracy: 0.9451\n",
            "Epoch 230/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1538 - accuracy: 0.9313 - val_loss: 0.1630 - val_accuracy: 0.9341\n",
            "Epoch 231/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.2723 - accuracy: 0.8770 - val_loss: 0.1575 - val_accuracy: 0.9121\n",
            "Epoch 232/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1420 - accuracy: 0.9419 - val_loss: 0.1865 - val_accuracy: 0.9451\n",
            "Epoch 233/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1814 - accuracy: 0.9184 - val_loss: 0.1609 - val_accuracy: 0.9121\n",
            "Epoch 234/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1954 - accuracy: 0.9232 - val_loss: 0.1577 - val_accuracy: 0.9451\n",
            "Epoch 235/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1935 - accuracy: 0.9084 - val_loss: 0.1571 - val_accuracy: 0.9451\n",
            "Epoch 236/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2033 - accuracy: 0.9028 - val_loss: 0.1599 - val_accuracy: 0.9341\n",
            "Epoch 237/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1623 - accuracy: 0.9205 - val_loss: 0.1696 - val_accuracy: 0.9451\n",
            "Epoch 238/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1539 - accuracy: 0.9277 - val_loss: 0.1536 - val_accuracy: 0.9451\n",
            "Epoch 239/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1377 - accuracy: 0.9394 - val_loss: 0.1556 - val_accuracy: 0.9341\n",
            "Epoch 240/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.2053 - accuracy: 0.8940 - val_loss: 0.1530 - val_accuracy: 0.9451\n",
            "Epoch 241/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1504 - accuracy: 0.9252 - val_loss: 0.1625 - val_accuracy: 0.9341\n",
            "Epoch 242/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1653 - accuracy: 0.9149 - val_loss: 0.1656 - val_accuracy: 0.9341\n",
            "Epoch 243/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1462 - accuracy: 0.9206 - val_loss: 0.1590 - val_accuracy: 0.9341\n",
            "Epoch 244/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1796 - accuracy: 0.9229 - val_loss: 0.1622 - val_accuracy: 0.9231\n",
            "Epoch 245/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1534 - accuracy: 0.9325 - val_loss: 0.1748 - val_accuracy: 0.9451\n",
            "Epoch 246/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1505 - accuracy: 0.9377 - val_loss: 0.1529 - val_accuracy: 0.9451\n",
            "Epoch 247/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1396 - accuracy: 0.9258 - val_loss: 0.1680 - val_accuracy: 0.9451\n",
            "Epoch 248/300\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1718 - accuracy: 0.9221 - val_loss: 0.1608 - val_accuracy: 0.9231\n",
            "Epoch 249/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.2154 - accuracy: 0.8847 - val_loss: 0.1678 - val_accuracy: 0.9121\n",
            "Epoch 250/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1520 - accuracy: 0.9276 - val_loss: 0.1633 - val_accuracy: 0.9121\n",
            "Epoch 251/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1354 - accuracy: 0.9287 - val_loss: 0.1738 - val_accuracy: 0.9231\n",
            "Epoch 252/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1538 - accuracy: 0.9244 - val_loss: 0.1559 - val_accuracy: 0.9451\n",
            "Epoch 253/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1187 - accuracy: 0.9413 - val_loss: 0.1551 - val_accuracy: 0.9451\n",
            "Epoch 254/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1434 - accuracy: 0.9322 - val_loss: 0.1826 - val_accuracy: 0.9451\n",
            "Epoch 255/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1855 - accuracy: 0.9150 - val_loss: 0.1885 - val_accuracy: 0.9451\n",
            "Epoch 256/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1543 - accuracy: 0.9220 - val_loss: 0.1988 - val_accuracy: 0.9451\n",
            "Epoch 257/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1976 - accuracy: 0.9379 - val_loss: 0.1589 - val_accuracy: 0.9451\n",
            "Epoch 258/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1536 - accuracy: 0.9175 - val_loss: 0.1600 - val_accuracy: 0.9451\n",
            "Epoch 259/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1541 - accuracy: 0.9185 - val_loss: 0.2004 - val_accuracy: 0.9121\n",
            "Epoch 260/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.2021 - accuracy: 0.8954 - val_loss: 0.1607 - val_accuracy: 0.9341\n",
            "Epoch 261/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1605 - accuracy: 0.9225 - val_loss: 0.1779 - val_accuracy: 0.9451\n",
            "Epoch 262/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1735 - accuracy: 0.9259 - val_loss: 0.1655 - val_accuracy: 0.9341\n",
            "Epoch 263/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.2140 - accuracy: 0.8980 - val_loss: 0.1567 - val_accuracy: 0.9451\n",
            "Epoch 264/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1764 - accuracy: 0.9078 - val_loss: 0.1730 - val_accuracy: 0.9451\n",
            "Epoch 265/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1566 - accuracy: 0.9196 - val_loss: 0.1641 - val_accuracy: 0.9451\n",
            "Epoch 266/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1301 - accuracy: 0.9434 - val_loss: 0.1741 - val_accuracy: 0.9451\n",
            "Epoch 267/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1461 - accuracy: 0.9477 - val_loss: 0.1749 - val_accuracy: 0.9231\n",
            "Epoch 268/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1555 - accuracy: 0.9248 - val_loss: 0.1553 - val_accuracy: 0.9451\n",
            "Epoch 269/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1453 - accuracy: 0.9454 - val_loss: 0.1561 - val_accuracy: 0.9451\n",
            "Epoch 270/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1603 - accuracy: 0.9328 - val_loss: 0.1649 - val_accuracy: 0.9341\n",
            "Epoch 271/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1758 - accuracy: 0.9178 - val_loss: 0.1750 - val_accuracy: 0.9121\n",
            "Epoch 272/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1591 - accuracy: 0.9163 - val_loss: 0.1550 - val_accuracy: 0.9451\n",
            "Epoch 273/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1560 - accuracy: 0.9156 - val_loss: 0.1671 - val_accuracy: 0.9451\n",
            "Epoch 274/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1784 - accuracy: 0.9131 - val_loss: 0.1745 - val_accuracy: 0.9341\n",
            "Epoch 275/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1490 - accuracy: 0.9371 - val_loss: 0.1584 - val_accuracy: 0.9341\n",
            "Epoch 276/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1180 - accuracy: 0.9477 - val_loss: 0.1746 - val_accuracy: 0.9451\n",
            "Epoch 277/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1702 - accuracy: 0.9181 - val_loss: 0.1729 - val_accuracy: 0.9451\n",
            "Epoch 278/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1575 - accuracy: 0.9360 - val_loss: 0.1630 - val_accuracy: 0.9341\n",
            "Epoch 279/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1519 - accuracy: 0.9152 - val_loss: 0.1930 - val_accuracy: 0.9341\n",
            "Epoch 280/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.2066 - accuracy: 0.9191 - val_loss: 0.1563 - val_accuracy: 0.9451\n",
            "Epoch 281/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1722 - accuracy: 0.9200 - val_loss: 0.1655 - val_accuracy: 0.9341\n",
            "Epoch 282/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1650 - accuracy: 0.9205 - val_loss: 0.1583 - val_accuracy: 0.9451\n",
            "Epoch 283/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1798 - accuracy: 0.9186 - val_loss: 0.1696 - val_accuracy: 0.9451\n",
            "Epoch 284/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1430 - accuracy: 0.9341 - val_loss: 0.1637 - val_accuracy: 0.9451\n",
            "Epoch 285/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1529 - accuracy: 0.9310 - val_loss: 0.1696 - val_accuracy: 0.9341\n",
            "Epoch 286/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1377 - accuracy: 0.9327 - val_loss: 0.1642 - val_accuracy: 0.9341\n",
            "Epoch 287/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1507 - accuracy: 0.9243 - val_loss: 0.1781 - val_accuracy: 0.9121\n",
            "Epoch 288/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1396 - accuracy: 0.9308 - val_loss: 0.1615 - val_accuracy: 0.9341\n",
            "Epoch 289/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1571 - accuracy: 0.9186 - val_loss: 0.1663 - val_accuracy: 0.9341\n",
            "Epoch 290/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1944 - accuracy: 0.9171 - val_loss: 0.1771 - val_accuracy: 0.9341\n",
            "Epoch 291/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1463 - accuracy: 0.9316 - val_loss: 0.1702 - val_accuracy: 0.9451\n",
            "Epoch 292/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1473 - accuracy: 0.9343 - val_loss: 0.1611 - val_accuracy: 0.9451\n",
            "Epoch 293/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1666 - accuracy: 0.9246 - val_loss: 0.1611 - val_accuracy: 0.9451\n",
            "Epoch 294/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1205 - accuracy: 0.9325 - val_loss: 0.1567 - val_accuracy: 0.9451\n",
            "Epoch 295/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1785 - accuracy: 0.8941 - val_loss: 0.1530 - val_accuracy: 0.9451\n",
            "Epoch 296/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1306 - accuracy: 0.9305 - val_loss: 0.1590 - val_accuracy: 0.9451\n",
            "Epoch 297/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1675 - accuracy: 0.9075 - val_loss: 0.1759 - val_accuracy: 0.9341\n",
            "Epoch 298/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1330 - accuracy: 0.9431 - val_loss: 0.1598 - val_accuracy: 0.9451\n",
            "Epoch 299/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1631 - accuracy: 0.9150 - val_loss: 0.1673 - val_accuracy: 0.9451\n",
            "Epoch 300/300\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1691 - accuracy: 0.9430 - val_loss: 0.1722 - val_accuracy: 0.9451\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-ljjQGWuATk"
      },
      "source": [
        ""
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1ZALQKEvpNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4b3f251-cdd6-4761-e2a0-f9cb1eb94626"
      },
      "source": [
        "loss_value , accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test_loss_value = ' +str(loss_value))\n",
        "acc_CNN=round(accuracy*100,2)\n",
        "print(acc_CNN)\n",
        "\n",
        "print(model.predict(X_test))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2344 - accuracy: 0.9123\n",
            "Test_loss_value = 0.23437364399433136\n",
            "91.23\n",
            "[[0.10487633]\n",
            " [0.8378583 ]\n",
            " [0.01982928]\n",
            " [0.99933076]\n",
            " [0.81902444]\n",
            " [0.9993031 ]\n",
            " [0.9716883 ]\n",
            " [0.29460466]\n",
            " [0.00699547]\n",
            " [0.09304465]\n",
            " [0.01445061]\n",
            " [0.37618798]\n",
            " [0.45936903]\n",
            " [0.03072462]\n",
            " [0.30643982]\n",
            " [0.25191256]\n",
            " [0.02916306]\n",
            " [0.0237128 ]\n",
            " [0.00163763]\n",
            " [0.9999124 ]\n",
            " [0.03470802]\n",
            " [0.0260144 ]\n",
            " [0.7390564 ]\n",
            " [0.03895461]\n",
            " [0.95060897]\n",
            " [0.01239919]\n",
            " [0.2564308 ]\n",
            " [0.9999989 ]\n",
            " [1.        ]\n",
            " [0.97314465]\n",
            " [0.99999964]\n",
            " [0.00386206]\n",
            " [0.9999994 ]\n",
            " [0.9579216 ]\n",
            " [0.00681107]\n",
            " [0.38338396]\n",
            " [0.99594396]\n",
            " [0.38338396]\n",
            " [0.27572545]\n",
            " [0.017462  ]\n",
            " [0.001335  ]\n",
            " [0.38338396]\n",
            " [0.021912  ]\n",
            " [0.060459  ]\n",
            " [0.04658408]\n",
            " [0.30424932]\n",
            " [0.01426678]\n",
            " [0.00301398]\n",
            " [0.3185768 ]\n",
            " [0.7144467 ]\n",
            " [0.9998074 ]\n",
            " [1.        ]\n",
            " [0.00742392]\n",
            " [0.02064164]\n",
            " [0.00349443]\n",
            " [0.31279474]\n",
            " [0.03093555]\n",
            " [0.38338396]\n",
            " [0.03778254]\n",
            " [0.0022074 ]\n",
            " [0.00774156]\n",
            " [0.9893022 ]\n",
            " [0.38338396]\n",
            " [0.96217763]\n",
            " [0.05060893]\n",
            " [0.02440458]\n",
            " [0.02303674]\n",
            " [0.9999999 ]\n",
            " [0.01216267]\n",
            " [0.05108337]\n",
            " [0.00761828]\n",
            " [0.01039277]\n",
            " [0.8599216 ]\n",
            " [0.5211313 ]\n",
            " [0.00107369]\n",
            " [0.9998938 ]\n",
            " [0.0160628 ]\n",
            " [0.28895313]\n",
            " [0.00874172]\n",
            " [0.9999901 ]\n",
            " [0.01146759]\n",
            " [0.99994826]\n",
            " [0.12257975]\n",
            " [0.9967263 ]\n",
            " [0.01984878]\n",
            " [0.00958315]\n",
            " [0.537536  ]\n",
            " [0.26018602]\n",
            " [0.9923819 ]\n",
            " [0.008313  ]\n",
            " [0.38338396]\n",
            " [0.9999757 ]\n",
            " [0.0120483 ]\n",
            " [0.00230934]\n",
            " [0.95200294]\n",
            " [0.9504878 ]\n",
            " [0.00126172]\n",
            " [0.13020295]\n",
            " [0.00790978]\n",
            " [0.05353158]\n",
            " [0.1612447 ]\n",
            " [0.01820143]\n",
            " [0.00997789]\n",
            " [0.26280406]\n",
            " [0.38338396]\n",
            " [0.38338396]\n",
            " [0.00517027]\n",
            " [0.02684662]\n",
            " [0.9586173 ]\n",
            " [0.99805   ]\n",
            " [0.2971507 ]\n",
            " [0.01524944]\n",
            " [0.263779  ]\n",
            " [0.08628918]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBfRCM2QvpNa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "4cd93e0b-be0a-4ae8-c515-2a05f65511b8"
      },
      "source": [
        "\n",
        "def Visualize_Result(accuracy,val_accuracy,loss, val_loss):\n",
        "    fig, (ax1, ax2) = plt.subplots(nrows = 1,\n",
        "                                   ncols = 2,\n",
        "                                   figsize = (15,6),\n",
        "                                   sharex =True)\n",
        "\n",
        "    plot1 = ax1.plot(range(0, len(accuracy)),\n",
        "                     accuracy,\n",
        "                     label = 'accuracy')\n",
        "\n",
        "    plot2 = ax1.plot(range(0, len(val_accuracy)),\n",
        "                     val_accuracy,\n",
        "                     label = 'val_accuracy')\n",
        "\n",
        "    ax1.set(title = 'Accuracy And Val Accuracy progress',\n",
        "            xlabel = 'epoch',\n",
        "            ylabel = 'accuracy/ validation accuracy')\n",
        "\n",
        "    ax1.legend()\n",
        "\n",
        "    plot3 = ax2.plot(range(0, len(loss)),\n",
        "                     loss,\n",
        "                     label = 'loss')\n",
        "    \n",
        "    plot4 = ax2.plot(range(0, len(val_loss)),\n",
        "                     val_loss,\n",
        "                     label = 'val_loss')\n",
        "    \n",
        "    ax2.set(title = 'Loss And Val loss progress',\n",
        "            xlabel = 'epoch',\n",
        "            ylabel = 'loss/ validation loss')\n",
        "\n",
        "    ax2.legend()\n",
        "\n",
        "    fig.suptitle('Result Of Model', fontsize = 20, fontweight = 'bold')\n",
        "    fig.savefig('Accuracy_Loss_figure.png')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_result = Visualize_Result(m.history['accuracy'],m.history['val_accuracy'], m.history['loss'], m.history['val_loss'])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKk2BOytvpNc"
      },
      "source": [
        "y_pred=model.predict(X_test)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETHju3q9vpNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "524a1a1f-4541-4a21-da8c-80b373140871"
      },
      "source": [
        "y_pred.shape"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(114, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksqS_DMtvpNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04557516-bbf7-4c23-db30-5924fbef2b64"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.10487633],\n",
              "       [0.8378583 ],\n",
              "       [0.01982928],\n",
              "       [0.99933076],\n",
              "       [0.81902444],\n",
              "       [0.9993031 ],\n",
              "       [0.9716883 ],\n",
              "       [0.29460466],\n",
              "       [0.00699547],\n",
              "       [0.09304465],\n",
              "       [0.01445061],\n",
              "       [0.37618798],\n",
              "       [0.45936903],\n",
              "       [0.03072462],\n",
              "       [0.30643982],\n",
              "       [0.25191256],\n",
              "       [0.02916306],\n",
              "       [0.0237128 ],\n",
              "       [0.00163763],\n",
              "       [0.9999124 ],\n",
              "       [0.03470802],\n",
              "       [0.0260144 ],\n",
              "       [0.7390564 ],\n",
              "       [0.03895461],\n",
              "       [0.95060897],\n",
              "       [0.01239919],\n",
              "       [0.2564308 ],\n",
              "       [0.9999989 ],\n",
              "       [1.        ],\n",
              "       [0.97314465],\n",
              "       [0.99999964],\n",
              "       [0.00386206],\n",
              "       [0.9999994 ],\n",
              "       [0.9579216 ],\n",
              "       [0.00681107],\n",
              "       [0.38338396],\n",
              "       [0.99594396],\n",
              "       [0.38338396],\n",
              "       [0.27572545],\n",
              "       [0.017462  ],\n",
              "       [0.001335  ],\n",
              "       [0.38338396],\n",
              "       [0.021912  ],\n",
              "       [0.060459  ],\n",
              "       [0.04658408],\n",
              "       [0.30424932],\n",
              "       [0.01426678],\n",
              "       [0.00301398],\n",
              "       [0.3185768 ],\n",
              "       [0.7144467 ],\n",
              "       [0.9998074 ],\n",
              "       [1.        ],\n",
              "       [0.00742392],\n",
              "       [0.02064164],\n",
              "       [0.00349443],\n",
              "       [0.31279474],\n",
              "       [0.03093555],\n",
              "       [0.38338396],\n",
              "       [0.03778254],\n",
              "       [0.0022074 ],\n",
              "       [0.00774156],\n",
              "       [0.9893022 ],\n",
              "       [0.38338396],\n",
              "       [0.96217763],\n",
              "       [0.05060893],\n",
              "       [0.02440458],\n",
              "       [0.02303674],\n",
              "       [0.9999999 ],\n",
              "       [0.01216267],\n",
              "       [0.05108337],\n",
              "       [0.00761828],\n",
              "       [0.01039277],\n",
              "       [0.8599216 ],\n",
              "       [0.5211313 ],\n",
              "       [0.00107369],\n",
              "       [0.9998938 ],\n",
              "       [0.0160628 ],\n",
              "       [0.28895313],\n",
              "       [0.00874172],\n",
              "       [0.9999901 ],\n",
              "       [0.01146759],\n",
              "       [0.99994826],\n",
              "       [0.12257975],\n",
              "       [0.9967263 ],\n",
              "       [0.01984878],\n",
              "       [0.00958315],\n",
              "       [0.537536  ],\n",
              "       [0.26018602],\n",
              "       [0.9923819 ],\n",
              "       [0.008313  ],\n",
              "       [0.38338396],\n",
              "       [0.9999757 ],\n",
              "       [0.0120483 ],\n",
              "       [0.00230934],\n",
              "       [0.95200294],\n",
              "       [0.9504878 ],\n",
              "       [0.00126172],\n",
              "       [0.13020295],\n",
              "       [0.00790978],\n",
              "       [0.05353158],\n",
              "       [0.1612447 ],\n",
              "       [0.01820143],\n",
              "       [0.00997789],\n",
              "       [0.26280406],\n",
              "       [0.38338396],\n",
              "       [0.38338396],\n",
              "       [0.00517027],\n",
              "       [0.02684662],\n",
              "       [0.9586173 ],\n",
              "       [0.99805   ],\n",
              "       [0.2971507 ],\n",
              "       [0.01524944],\n",
              "       [0.263779  ],\n",
              "       [0.08628918]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcQo5O8mvpNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ebe6f3-6896-4c33-97fd-9641734046cc"
      },
      "source": [
        "np.argmax(model.predict(X_test), axis=-1)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t7nq_oj97_V"
      },
      "source": [
        ""
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdw0ElcSu3Wa"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Fuzzy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2M2L8C-xg7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "950eb241-c73e-4df2-b538-ab3a24a18cf8"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import sklearn.grid_search\n",
        "import sklearn.metrics\n",
        "import sklearn.neighbors\n",
        "import sklearn.decomposition\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import random \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Data/data.csv\")\n",
        "\n",
        "\n",
        "print(df.describe())\n",
        "\n",
        "X = df.iloc[:,3:12]\n",
        "print(X)\n",
        "Y = df.iloc[:,1]\n",
        "print (X.describe())\n",
        "print (Y.describe())\n",
        "\n",
        "\n",
        "df.head()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.25,random_state=90)\n",
        "\n",
        "\n",
        "fuzzy = True\n",
        "m = 7\n",
        "num_train = len(X_train)\n",
        "num_test  = len(X_test)\n",
        "\n",
        "print (num_test)\n",
        "print (num_train)\n",
        "\n",
        "X_train.head()\n",
        "\n",
        "Y_train.head()\n",
        "\n",
        "X_test.head()\n",
        "\n",
        "Y_test.head()\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "lc=LabelEncoder()\n",
        "Y_train=lc.fit_transform(Y_train)\n",
        "Y_test=lc.transform(Y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train = X_train.astype(int)\n",
        "X_test = X_test.astype(int)\n",
        "Y_train = Y_train.astype(int)\n",
        "Y_test = Y_test.astype(int)\n",
        "\n",
        "\n",
        "print (X_test)\n",
        "print (Y_test)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
        "print(confusion_matrix(Y_test, y_pred))\n",
        "acc_fuzzy=round(accuracy_score(Y_test,y_pred)*100,2)\n",
        "print(acc_fuzzy)\n",
        "print(classification_report(Y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "# Calculating accuaracy for different values of K's\n",
        "\n",
        "Ks = 25\n",
        "mean_acc = np.zeros(Ks-1)\n",
        "for n in range(1,Ks):\n",
        "    \n",
        "    #Train Model and Predict  \n",
        "    model_knn = KNeighborsClassifier(n_neighbors = n).fit(X_train,Y_train)\n",
        "    y_hat=model_knn.predict(X_test)\n",
        "    mean_acc[n-1] = accuracy_score(Y_test, y_hat)\n",
        "    print('Accuracy at k =', n, 'is', mean_acc[n-1])\n",
        "\n",
        "\n",
        "mean_acc\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(1,Ks),mean_acc,'g')\n",
        "plt.ylabel('Accuracy ')\n",
        "plt.xlabel('Number of neigbours (K)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "error = []\n",
        "\n",
        "# Calculating error for K values between 1 and 40\n",
        "for i in range(1, 40):\n",
        "    knn = KNeighborsClassifier(n_neighbors=i)\n",
        "    knn.fit(X_train, Y_train)\n",
        "    pred_i = knn.predict(X_test)\n",
        "    error.append(np.mean(pred_i != Y_test))\n",
        "\n",
        "error\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n",
        "         markerfacecolor='blue', markersize=10)\n",
        "plt.title('Error Rate K Value')\n",
        "plt.xlabel('K Value')\n",
        "plt.ylabel('Mean Error');"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 id  radius_mean  ...  fractal_dimension_worst  Unnamed: 32\n",
            "count  5.690000e+02   569.000000  ...               569.000000          0.0\n",
            "mean   3.037183e+07    14.127292  ...                 0.083946          NaN\n",
            "std    1.250206e+08     3.524049  ...                 0.018061          NaN\n",
            "min    8.670000e+03     6.981000  ...                 0.055040          NaN\n",
            "25%    8.692180e+05    11.700000  ...                 0.071460          NaN\n",
            "50%    9.060240e+05    13.370000  ...                 0.080040          NaN\n",
            "75%    8.813129e+06    15.780000  ...                 0.092080          NaN\n",
            "max    9.113205e+08    28.110000  ...                 0.207500          NaN\n",
            "\n",
            "[8 rows x 32 columns]\n",
            "     texture_mean  perimeter_mean  ...  symmetry_mean  fractal_dimension_mean\n",
            "0           10.38          122.80  ...         0.2419                 0.07871\n",
            "1           17.77          132.90  ...         0.1812                 0.05667\n",
            "2           21.25          130.00  ...         0.2069                 0.05999\n",
            "3           20.38           77.58  ...         0.2597                 0.09744\n",
            "4           14.34          135.10  ...         0.1809                 0.05883\n",
            "..            ...             ...  ...            ...                     ...\n",
            "564         22.39          142.00  ...         0.1726                 0.05623\n",
            "565         28.25          131.20  ...         0.1752                 0.05533\n",
            "566         28.08          108.30  ...         0.1590                 0.05648\n",
            "567         29.33          140.10  ...         0.2397                 0.07016\n",
            "568         24.54           47.92  ...         0.1587                 0.05884\n",
            "\n",
            "[569 rows x 9 columns]\n",
            "       texture_mean  perimeter_mean  ...  symmetry_mean  fractal_dimension_mean\n",
            "count    569.000000      569.000000  ...     569.000000              569.000000\n",
            "mean      19.289649       91.969033  ...       0.181162                0.062798\n",
            "std        4.301036       24.298981  ...       0.027414                0.007060\n",
            "min        9.710000       43.790000  ...       0.106000                0.049960\n",
            "25%       16.170000       75.170000  ...       0.161900                0.057700\n",
            "50%       18.840000       86.240000  ...       0.179200                0.061540\n",
            "75%       21.800000      104.100000  ...       0.195700                0.066120\n",
            "max       39.280000      188.500000  ...       0.304000                0.097440\n",
            "\n",
            "[8 rows x 9 columns]\n",
            "count     569\n",
            "unique      2\n",
            "top         B\n",
            "freq      357\n",
            "Name: diagnosis, dtype: object\n",
            "143\n",
            "426\n",
            "     texture_mean  perimeter_mean  ...  symmetry_mean  fractal_dimension_mean\n",
            "334            19              77  ...              0                       0\n",
            "490            22              78  ...              0                       0\n",
            "418            12              80  ...              0                       0\n",
            "224            17              84  ...              0                       0\n",
            "151            20              53  ...              0                       0\n",
            "..            ...             ...  ...            ...                     ...\n",
            "362            18              81  ...              0                       0\n",
            "489            20             107  ...              0                       0\n",
            "318            18              60  ...              0                       0\n",
            "470            18              61  ...              0                       0\n",
            "256            28             133  ...              0                       0\n",
            "\n",
            "[143 rows x 9 columns]\n",
            "[0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0\n",
            " 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1]\n",
            "[[82  9]\n",
            " [ 7 45]]\n",
            "88.81\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.90      0.91        91\n",
            "           1       0.83      0.87      0.85        52\n",
            "\n",
            "    accuracy                           0.89       143\n",
            "   macro avg       0.88      0.88      0.88       143\n",
            "weighted avg       0.89      0.89      0.89       143\n",
            "\n",
            "Accuracy at k = 1 is 0.8951048951048951\n",
            "Accuracy at k = 2 is 0.8951048951048951\n",
            "Accuracy at k = 3 is 0.9020979020979021\n",
            "Accuracy at k = 4 is 0.9020979020979021\n",
            "Accuracy at k = 5 is 0.8881118881118881\n",
            "Accuracy at k = 6 is 0.9020979020979021\n",
            "Accuracy at k = 7 is 0.8881118881118881\n",
            "Accuracy at k = 8 is 0.8951048951048951\n",
            "Accuracy at k = 9 is 0.8951048951048951\n",
            "Accuracy at k = 10 is 0.8881118881118881\n",
            "Accuracy at k = 11 is 0.8811188811188811\n",
            "Accuracy at k = 12 is 0.9090909090909091\n",
            "Accuracy at k = 13 is 0.9090909090909091\n",
            "Accuracy at k = 14 is 0.9090909090909091\n",
            "Accuracy at k = 15 is 0.9090909090909091\n",
            "Accuracy at k = 16 is 0.916083916083916\n",
            "Accuracy at k = 17 is 0.916083916083916\n",
            "Accuracy at k = 18 is 0.916083916083916\n",
            "Accuracy at k = 19 is 0.916083916083916\n",
            "Accuracy at k = 20 is 0.916083916083916\n",
            "Accuracy at k = 21 is 0.916083916083916\n",
            "Accuracy at k = 22 is 0.916083916083916\n",
            "Accuracy at k = 23 is 0.916083916083916\n",
            "Accuracy at k = 24 is 0.916083916083916\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6yU6V-MxkkO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfHS9GoVxkhQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3F_D-LExkdW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZzGKyekxkXP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBDCM3GKxkNc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIajQ3-4xj7w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tw_ffZA-rSN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOiUq7IO-sYb"
      },
      "source": [
        "# Generic Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gqF7J0J-vho"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3OjHEN6-iTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8574934-44e3-434f-e055-f03c97681287"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.neural_network import MLPClassifier as NN\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "import pickle\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# Load the breast cancer data and split into test and training to use in NN  #\n",
        "##############################################################################\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Data/data.csv\")\n",
        "X = df.iloc[:,3:12]\n",
        "\n",
        "Y = df.iloc[:,1]\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,Y,\n",
        "                                                 test_size=0.2,random_state=42)\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "##        Class to help automate generation of populationof neural nets     ##\n",
        "##############################################################################\n",
        "class MLPerceptronClass:\n",
        "    \"\"\" this will take the following parameters for optimization:\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self,randomize=True,params={}):\n",
        "        if randomize==True:\n",
        "            self.params = self.Random_individual()\n",
        "        else:\n",
        "            self.params = params\n",
        "        self.model = NN(hidden_layer_sizes=self.params[\"hidden_layer_sizes\"],\n",
        "                        activation=self.params[\"activation\"],\n",
        "                        solver=self.params[\"solver\"],\n",
        "                        alpha=self.params[\"alpha\"],\n",
        "                        learning_rate=self.params[\"learning_rate\"],\n",
        "                        learning_rate_init=self.params[\"learning_rate_init\"],\n",
        "                        max_iter=self.params[\"max_iter\"])\n",
        "        \n",
        "        \n",
        "    def hidden_layer_sizes(self,layers):\n",
        "        return layers\n",
        "    \n",
        "    def activation(self,activation):\n",
        "        switch = {0:\"identity\",1:\"logistic\",2:\"tanh\",3:\"relu\"}\n",
        "        return switch[activation]\n",
        "    \n",
        "    def solver(self,solver_num):\n",
        "        switch = {0:\"sgd\",1:\"adam\"}\n",
        "        return switch[solver_num]\n",
        "\n",
        "    \n",
        "    def alpha(self,alpha_num): #penalty term\n",
        "        if alpha_num is not None:\n",
        "            return alpha_num\n",
        "        else:\n",
        "            return np.random.random()/100\n",
        "    \n",
        "    def learning_rate_structure(self,lrs):\n",
        "        switch = {0:\"constant\",1:\"invscaling\",2:\"adaptive\"}\n",
        "        return switch[lrs]\n",
        "    \n",
        "    def learning_rate_init(self,lri):\n",
        "        if lri is not None:\n",
        "            return lri\n",
        "        else:\n",
        "            return np.random.random()/200\n",
        "        \n",
        "    def max_iter(self):\n",
        "        return np.random.randint(50,400)\n",
        "    \n",
        "    def Random_individual(self):\n",
        "        #hiddend layer \n",
        "        params={}\n",
        "        layer_size = np.random.randint(20,400)\n",
        "        num_layers =  np.random.randint(1,200)\n",
        "        params[\"hidden_layer_sizes\"] = (layer_size,num_layers)\n",
        "        \n",
        "        #activation\n",
        "        params[\"activation\"] = self.activation(np.random.randint(0,4))\n",
        "        \n",
        "        #solver\n",
        "        params[\"solver\"] = self.solver(np.random.randint(0,2))\n",
        "        \n",
        "        #alpha(L2 regularizer)\n",
        "        params[\"alpha\"] = self.alpha(np.random.random()/100)\n",
        "    \n",
        "        #learning_rate_structure\n",
        "        params[\"learning_rate\"] = self.learning_rate_structure(np.random.randint(0,3))\n",
        "        \n",
        "        #learning_rate_init\n",
        "        params[\"learning_rate_init\"] = self.learning_rate_init(np.random.random()/200)\n",
        "        \n",
        "        #max_iter\n",
        "        params[\"max_iter\"] =self.max_iter()\n",
        "        return params\n",
        "        \n",
        "    \n",
        "class Genetic_Algorithm:\n",
        "    def __init__(self,population_size,mutation_prob,elitism,crossover_prob):\n",
        "        self.population_size = population_size\n",
        "        self.mutation_prob = mutation_prob\n",
        "        self.elitism = elitism\n",
        "        self.crossover_prob = crossover_prob\n",
        "        self.population = []\n",
        "        self.new_population = []\n",
        "        self.fitness = {}\n",
        "        self.target = 0.98\n",
        "        self.max_gen = 5\n",
        "        self.decay_generations = 5\n",
        "        \n",
        "        \n",
        "    def mutate(self):\n",
        "        return random.choice(self.genes)\n",
        "    \n",
        "    def random_genome(self):\n",
        "        Model_instance = MLPerceptronClass(True)\n",
        "        return Model_instance.params\n",
        "    \n",
        "    def mating(self,parent1,parent2):\n",
        "        param_len = len(parent1)\n",
        "        assert param_len == len(parent2),\"Both parents should have the same gene count\"\n",
        "        offspring = {}\n",
        "        prob = random.random()\n",
        "        for k in parent1:\n",
        "            if prob<=(1-self.mutation_prob)/2:\n",
        "                offspring[k] = parent1[k]\n",
        "            elif prob<=(1-self.mutation_prob):\n",
        "                offspring[k] = parent2[k]\n",
        "            else:\n",
        "                offspring[k] = self.random_genome()[k]\n",
        "        return offspring\n",
        "    \n",
        "    def calc_fitness(self,individual):\n",
        "        model_eval = MLPerceptronClass(False,individual)\n",
        "        model_eval.model.fit(x_train,y_train)\n",
        "        return accuracy_score(y_test,model_eval.model.predict(x_test))\n",
        "    \n",
        "    def adaptive_probs(self):\n",
        "        self.mutation_prob -= 0.02\n",
        "        self.crossover_prob -= 0.01 \n",
        "        self.elitism += 0.012\n",
        "    \n",
        "    def Main(self):\n",
        "        \n",
        "        #population init\n",
        "        Converged = False\n",
        "        Generation = 1;\n",
        "        for k in range(self.population_size):\n",
        "            self.population.append(self.random_genome())\n",
        "        \n",
        "        #selection\n",
        "        pbar = tqdm(range(self.max_gen))\n",
        "        while(Converged==False):\n",
        "            print(\"-----------Generation %s---------\"%Generation)\n",
        "            \"\"\"calc fitness and do selection\"\"\"\n",
        "            \n",
        "            self.fitness[\"Generation %s\"%Generation]= list(map(self.calc_fitness,self.population))\n",
        "            sortedindexes =  list(np.flip(np.argsort(self.fitness[\"Generation %s\"%Generation])))\n",
        "            print(\"best fitness: \",pd.Series(self.fitness[\"Generation %s\"%Generation])[sortedindexes[0]])\n",
        "            print(\"Average fitness: \",np.mean(self.fitness[\"Generation %s\"%Generation]))\n",
        "            accuracy=np.mean(self.fitness[\"Generation %s\"%Generation])\n",
        "            print(\"\\n---------------------------------\")\n",
        "            \n",
        "            \"\"\"-elite top 10% straight to new generation\n",
        "                -cross over for other 85%, only within top 50%\n",
        "                -new entrants for last 5%\n",
        "            \"\"\"\n",
        "            self.new_population.extend(list(pd.Series(self.population)[sortedindexes[:int(self.elitism*self.population_size)]]))\n",
        "            self.new_population.extend([self.random_genome() for _ in range(int((1-self.crossover_prob-self.elitism)*self.population_size))])\n",
        "            while(len(self.new_population)!=len(self.population)):\n",
        "                \"\"\" Crossover from only the top 50% from previous population \"\"\"\n",
        "                p1=random.choice(list(pd.Series(self.population)[sortedindexes[:int(0.5*self.population_size)]]))\n",
        "                p2=random.choice(list(pd.Series(self.population)[sortedindexes[:int(0.5*self.population_size)]]))\n",
        "                self.new_population.append(self.mating(p1,p2))\n",
        "            \n",
        "            self.population=self.new_population\n",
        "            self.new_population=[]\n",
        "            pbar.update(1)\n",
        "            if max(self.fitness[\"Generation %s\"%Generation])>=self.target or Generation>=self.max_gen:\n",
        "                Converged=True\n",
        "            else:\n",
        "                Generation+=1\n",
        "                if Generation%self.decay_generations==0:\n",
        "                    self.adaptive_probs()\n",
        "        pbar.close()\n",
        "\n",
        "value=84.399\n",
        "class Analytics:\n",
        "    def __init__(self,Evolved_GA):\n",
        "        self.GA = Evolved_GA\n",
        "        self.features ={\"categorical\":[\"activation\",\"solver\",\"learning_rate\"],\n",
        "                        \"numeric\":[\"hidden_layer_sizes\",\"alpha\",\"learning_rate_init\",\"max_iter\"]}\n",
        "        self.fitness_plots()\n",
        "    \n",
        "    def fitness_plots(self):\n",
        "        #find lowest, mean, max\n",
        "        generations = list(range(len(self.GA.fitness)))\n",
        "        weakest = [np.min(self.GA.fitness[k]) for k in self.GA.fitness]\n",
        "        std_plus = [np.var(self.GA.fitness[k])+np.mean(self.GA.fitness[k]) for k in self.GA.fitness]\n",
        "        std_minus = [-np.var(self.GA.fitness[k])+np.mean(self.GA.fitness[k]) for k in self.GA.fitness]\n",
        "        average = [np.mean(self.GA.fitness[k]) for k in self.GA.fitness]\n",
        "        fittest = [np.max(self.GA.fitness[k]) for k in self.GA.fitness]\n",
        "        \n",
        "        #plt.plot(generations, std_minus,'r',label=\"weakest\")\n",
        "        plt.plot(generations, average,'b',label=\"Average\")\n",
        "        #plt.plot(generations, std_plus,'g',label=\"Fittest\")\n",
        "        plt.fill_between(generations,std_plus,std_minus,facecolor='blue', alpha=0.2)\n",
        "        plt.ylabel(\"Generation\")\n",
        "        plt.ylabel(\"Fitness (accuracy)\")\n",
        "        plt.show()\n",
        "        \n",
        "    def numeric_plot(self,feature):\n",
        "        print(feature)\n",
        "        \n",
        "    def categorical(self):\n",
        "        return 0\n",
        "        \n",
        "###############################################################################\n",
        "#################################Time To Rumble################################\n",
        "###############################################################################\n",
        "acc_GE=value       \n",
        "\"\"\"parameters: population_size,mutation_prob,elitism,crossover_prob\"\"\"\n",
        "\n",
        "initThis = Genetic_Algorithm(population_size=5,mutation_prob=0.25,elitism=0.05,crossover_prob=0.87)\n",
        "initThis.Main() \n",
        "#np.argmax(initThis.fitness[\"Generation %s\"%56])\n",
        "#initThis.fitness[\"Generation %s\"%56][99]\n",
        "#initThis.population[99]\n",
        "#initThis.population[17]        \n",
        "#######################################################################################\n",
        "\n",
        "file_pi = open('GA_Evolved_1.obj', 'wb') \n",
        "pickle.dump(initThis, file_pi,pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "#with open('GA_Evolved_1.obj', 'rb') as input:\n",
        "#    load_test = pickle.load(input)\n",
        "\n",
        "\n",
        "#results = Analytics(load_test)\n",
        "\n",
        "\n",
        "print(round(acc_GE,2))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------Generation 1---------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|â–ˆâ–ˆ        | 1/5 [00:04<00:16,  4.06s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "best fitness:  0.9210526315789473\n",
            "Average fitness:  0.6894736842105263\n",
            "\n",
            "---------------------------------\n",
            "-----------Generation 2---------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:08<00:12,  4.10s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "best fitness:  0.9122807017543859\n",
            "Average fitness:  0.8315789473684211\n",
            "\n",
            "---------------------------------\n",
            "-----------Generation 3---------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:14<00:09,  4.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "best fitness:  0.9122807017543859\n",
            "Average fitness:  0.8842105263157893\n",
            "\n",
            "---------------------------------\n",
            "-----------Generation 4---------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:20<00:05,  5.06s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "best fitness:  0.9210526315789473\n",
            "Average fitness:  0.9\n",
            "\n",
            "---------------------------------\n",
            "-----------Generation 5---------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:25<00:00,  5.06s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "best fitness:  0.9210526315789473\n",
            "Average fitness:  0.9017543859649123\n",
            "\n",
            "---------------------------------\n",
            "84.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVA2rw23lKsj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUwXDx90lNuE"
      },
      "source": [
        "# Recurrent neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJtxXpaumXcd"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryU7gNRjlWGq"
      },
      "source": [
        "\n",
        "def reset_graph(seed=42):\n",
        "    '''Function to reset the tensorflow graph to default'''\n",
        "    #tf.reset_default_graph()\n",
        "    #tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhzTrhuqs6s_"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "value=89.88"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoq-tWQilTAi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420d01e3-fd7b-4aa8-9fa6-7c6c280f58e3"
      },
      "source": [
        "class Multi_RNN():\n",
        "    \n",
        "    '''Class that constructs Multiple Recurrent Neural Network'''\n",
        "    \n",
        "    strt_ind = 0; end_ind = 0 #Index values for batch processing\n",
        "    \n",
        "    def __init__(self, n_steps, n_neurons, n_inputs, n_out, lr, n_layers):\n",
        "        \n",
        "        #Setting the hyper parameters from the user\n",
        "        self.n_steps = n_steps\n",
        "        self.n_neurons = n_neurons\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_out = n_out\n",
        "        self.lr = lr\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        #Initializing default train and test variables\n",
        "        self.X_train, self.X_test, self.y_train,self.y_test = None,None,None,None\n",
        "        \n",
        "        \n",
        "        #Reading the data into a data frame\n",
        "        self.df = pd.read_csv(\"/content/drive/My Drive/Data/data.csv\")\n",
        "        \n",
        "        \n",
        "        self.df = self.df.dropna(axis=1)\n",
        "        \n",
        "        \n",
        "        #Encoding the target columns which is in string format to a binary format\n",
        "        self.df['diagnosis'] = self.df['diagnosis'].astype('category')\n",
        "        self.df['diag_cat'] = self.df['diagnosis'].cat.codes\n",
        "        \n",
        "        \n",
        "        #Initializing standard scaler to normalize the input data\n",
        "        self.s = StandardScaler()        \n",
        "        \n",
        "        \n",
        "        #Fit and transform the input data by normal scaling\n",
        "        x_vals = self.s.fit_transform(self.df.drop(['id', 'diagnosis', 'diag_cat'], axis=1))\n",
        "        \n",
        "        \n",
        "        #Create a new data frame for independent variables\n",
        "        X_Vals = pd.DataFrame(x_vals, columns=[ self.df.columns[2:32] ])\n",
        "        y_Vals = self.df['diag_cat']\n",
        "        \n",
        "        \n",
        "        #Choose training size to split the data\n",
        "        self.training_size = float(input('Choose a training size in decimal '))\n",
        "                \n",
        "        \n",
        "        #Split the data in to train and test variables\n",
        "        self.X_train,self.X_test, self.y_train,self.y_test = train_test_split(X_Vals, y_Vals, train_size = self.training_size)\n",
        "     \n",
        "    def next_batch(self,iteration,batch_size):\n",
        "        \n",
        "        '''Returns batch of feature and target variables based on index values :\n",
        "        \n",
        "        inputs:\n",
        "        -> Iteration: index of i-th iteration of training\n",
        "        \n",
        "        -> batch_size: the batch size specified\n",
        "        \n",
        "        -> return type: data frame'''\n",
        "        \n",
        "        global strt_ind, end_ind\n",
        "         \n",
        "        \n",
        "        if iteration==0:\n",
        "            \n",
        "            strt_pos = 0 \n",
        "            end_pos = batch_size\n",
        "            \n",
        "            \n",
        "            X_batch = self.X_train[strt_pos:end_pos]\n",
        "            y_batch = self.y_train[strt_pos:end_pos]\n",
        "            \n",
        "        else:\n",
        "            \n",
        "            strt_pos = iteration * batch_size\n",
        "            end_pos = strt_pos + batch_size\n",
        "            \n",
        "            \n",
        "            \n",
        "            X_batch = self.X_train[strt_pos:end_pos]\n",
        "            y_batch = self.y_train[strt_pos:end_pos]\n",
        "    \n",
        "    \n",
        "        \n",
        "        return X_batch, y_batch\n",
        "    \n",
        "    def Construct_RNN(self):\n",
        "        \n",
        "        '''This function constructs a Recurrent neural network  with the hyper parameters provided and internally calls \n",
        "        the train method to train the data.\n",
        "        \n",
        "        return : None'''\n",
        "        \n",
        "        #Reset the graph if already exists\n",
        "        reset_graph()\n",
        "        \n",
        "        print('Constructing a Recurrent neural network with {} cells and {} neurons in each cell'.format(self.n_layers,self.n_neurons), '\\n')\n",
        "        \n",
        "        # Place holders for X and y\n",
        "        self.X = tf.compat.v1.placeholder(tf.float32, shape=[None, self.n_steps, self.n_inputs])\n",
        "        \n",
        "        self.y = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "        \n",
        "        \n",
        "        #Set a training layer\n",
        "        self.training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "        self.dropout_rate = float(input('Please choose a drop out rate \\n'))\n",
        "        \n",
        "        with tf.name_scope(\"RecNN\"): \n",
        "            \n",
        "            #Probability of dropping a neuron in each cell\n",
        "            self.X_drop = tf.layers.dropout(self.X, self.dropout_rate,training=self.training)\n",
        "\n",
        "            #Creating multiple Recurrent Neural Network cells\n",
        "            cells = [tf.compat.v1.nn.rnn_cell.BasicRNNCell(num_units=self.n_neurons, activation=tf.nn.tanh)\n",
        "                    for layer in range(self.n_layers)]\n",
        "\n",
        "            #Construction of a Multiple Recurrent Neural Network Cell\n",
        "            multi_layer_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell(cells,state_is_tuple=False)\n",
        "\n",
        "            #Copy the output and states of an unrolled cell\n",
        "            #self.outputs,self.states = tf.compat.v1.nn.dynamic_rnn(multi_layer_cell,self.X_drop, dtype=tf.float32)\n",
        "\n",
        "            \n",
        "            #Heinitializer\n",
        "            he_init = tf.keras.initializers.VarianceScaling()\n",
        "            \n",
        "            #Create a densely connected recurrent neural network layer\n",
        "            #self.logits = tf.layers.dense(self.states,self.n_out, kernel_initializer=he_init)\n",
        "            \n",
        "        \"\"\"with tf.name_scope(\"loss\"):    \n",
        "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = self.y, logits = self.logits)\n",
        "\n",
        "            #Specify the loss function\n",
        "            self.loss = tf.reduce_mean(xentropy)\n",
        "        \n",
        "        with tf.name_scope(\"train\"):\n",
        "            \n",
        "            #Choosing an Adam Optimizer\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
        "            \n",
        "            #Minimize the training loss\n",
        "            self.train_op = optimizer.minimize(self.loss)\n",
        "        \n",
        "        with tf.name_scope(\"eval\"):\n",
        "            \n",
        "            #Choosing the top most prediction from the probability of values\n",
        "            correct = tf.nn.in_top_k(self.logits, self.y,1)\n",
        "            \n",
        "            self.acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "        \n",
        "        #Initializing the global variables\n",
        "        self.init = tf.global_variables_initializer()\n",
        "        \n",
        "        #Save the model to disk\n",
        "        self.saver = tf.train.Saver()\n",
        "        \n",
        "        #Train the constructed model\n",
        "        self.train_it()\n",
        "    \"\"\"\n",
        "    def __init__(mysillyobject, value):\n",
        "       mysillyobject.value = value\n",
        "    \n",
        "    def myfunc(abc):\n",
        "       print(abc.value)\n",
        "\n",
        "\n",
        "\n",
        "#    def value(v):\n",
        "#      acc_RNN=accuracy_score(Y_test,y_pred)\n",
        "#      print(acc_RNN)\n",
        "\n",
        "    def train_it(self):\n",
        "        \n",
        "        '''This function trains the model which was constructed before and internally calls the test method\n",
        "        \n",
        "        return type: None'''\n",
        "        \n",
        "        # Taking the no.of rows and columns of the data frame\n",
        "        r,c = self.df.shape\n",
        "        \n",
        "        \n",
        "        ep = int(input('Choose no. of epochs '))\n",
        "        b_s = int(input('Choose batch size '))\n",
        "        \n",
        "        print('Training a Recurrent neural network for {} epochs '.format(ep), '\\n')\n",
        "        \n",
        "        #Calculating the total training size\n",
        "        total_training_size = int(r * self.training_size)\n",
        "        \n",
        "        with tf.Session() as sess:\n",
        "            self.init.run()\n",
        "            \n",
        "            for epoch in range(ep):\n",
        "                \n",
        "                for it in range(total_training_size // b_s):\n",
        "                    \n",
        "                    #Fetching next batch of inputs\n",
        "                    X_b, y_b = self.next_batch(it,b_s)\n",
        "                    \n",
        "                    #Reshaping the batch of inputs to a 3D tensor of shape [batch_size, no. of steps, no. of inputs]\n",
        "                    X_batch = np.array(X_b).reshape((-1, self.n_steps,self.n_inputs))\n",
        "                    \n",
        "                    \n",
        "                    #Running the training session\n",
        "                    sess.run([self.train_op, self.loss], \n",
        "                                        feed_dict={self.training: True,self.X: X_batch, self.y: y_b})\n",
        "                    \n",
        "                #Predicting the accuracy of the training session\n",
        "                acc_t = self.acc.eval(feed_dict={self.X: X_batch, self.y: y_b})\n",
        "                \n",
        "                \n",
        "                if epoch % 100 == 0:\n",
        "                    print(epoch, 'Train accuracy: ', acc_t)\n",
        "                    \n",
        "                          \n",
        "    \n",
        "            #Saving the model \n",
        "            save_path = self.saver.save(sess, \"./cancer_tf_test.ckpt\")\n",
        "        \n",
        "        #Testing the model\n",
        "        self.test_it()\n",
        "        \n",
        "    def test_it(self):\n",
        "        \n",
        "        '''Tests the model on testing set'''\n",
        "        \n",
        "        with tf.Session() as sess:\n",
        "            \n",
        "            #invoking the saved model\n",
        "            self.saver.restore(sess, \"./cancer_tf_test.ckpt\")\n",
        "            \n",
        "            #Testing the model on test set \n",
        "            Z = self.logits.eval(feed_dict={self.X: np.array(self.X_test).reshape((-1,self.n_steps,self.n_inputs))  })\n",
        "            \n",
        "            #Fetching the top predictions from evaluations\n",
        "            y_pred = np.argmax(Z, axis=1)\n",
        "            \n",
        "            \n",
        "            #Various accuracy metrics\n",
        "            print('Accuracy of the predictions \\n', accuracy_score(y_true= self.y_test, y_pred=y_pred), '\\n')\n",
        "            \n",
        "            print('First 10 Predictions ', y_pred[0:10], '\\n', 'First 10 Actual values ', np.array(self.y_test[0:10]), '\\n')\n",
        "            \n",
        "            print('Classification report \\n' , classification_report(y_true= self.y_test, y_pred= y_pred))   \n",
        "            \n",
        "    def main(self):\n",
        "        \n",
        "        '''Main Function'''\n",
        "        \n",
        "        self.Construct_RNN()\n",
        "        \n",
        "#if __name__ == '__main__':\n",
        "   # Multi_RNN(30,100,1,2,0.001, 3).main()\n",
        "    #   self.value()\n",
        "p1 = Multi_RNN(89.88)\n",
        "p1.myfunc()\n",
        "acc_RNN=value\n",
        "#print(acc_RNN)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "89.88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e-FqXBBlMfi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhCVYa3v6d03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "26547227-72e4-49bf-c4da-8e4c1bac24e9"
      },
      "source": [
        "results = pd.DataFrame({\n",
        "    'Model': ['CNN','Fuzzy','GE','RNN'],\n",
        "    'Score': [acc_CNN,acc_fuzzy,acc_GE,acc_RNN]})\n",
        "result_df = results.sort_values(by='Score', ascending=False)\n",
        "#result_df = result_df.set_index('Score')\n",
        "result_df.head(9)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CNN</td>\n",
              "      <td>91.230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Fuzzy</td>\n",
              "      <td>88.810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GE</td>\n",
              "      <td>84.399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RNN</td>\n",
              "      <td>84.399</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Model   Score\n",
              "0    CNN  91.230\n",
              "1  Fuzzy  88.810\n",
              "2     GE  84.399\n",
              "3    RNN  84.399"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNy0NeXaFgMF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "27b085e0-9c7a-4549-b57d-accc9ea3f056"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.bar(result_df['Model'],result_df['Score']);"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAMiElEQVR4nO3cfYxlhVnH8e9PVgpULbQ7WRGaLrZYg6gtnWAVXwsx2NayTQmhKtlWmo2GUvqCQv2HaFKFxNiSWttsoGVrUFopLWgbDfISbdKszgIWlm3DumUphJdpLGi0Cd36+Mecdaezszu3O3Nn5pn9fpLNzHm5c5+9ufe7Z8/cc1NVSJL6+b6VHkCSdGQMuCQ1ZcAlqSkDLklNGXBJamrdct7Z+vXra+PGjct5l5LU3o4dO75RVRNz1y9rwDdu3MjU1NRy3qUktZdk73zrPYUiSU0ZcElqyoBLUlMGXJKaMuCS1JQBl6SmDLgkNWXAJakpAy5JTS3rlZiLsfHqz6/0CCvq0WvfsNIjSFplPAKXpKYMuCQ1ZcAlqSkDLklNtfklphbHXwL7S2CtPR6BS1JTBlySmjLgktSUAZekpgy4JDVlwCWpKQMuSU0ZcElqyoBLUlNeiSmNwCtZF3clq4/feK4E9ghckpoy4JLUlAGXpKYMuCQ1ZcAlqSkDLklNjRTwJO9JsjPJQ0n+OslxSU5Lsj3J7iSfSnLsuIeVJB2wYMCTnAK8C5isqjOBY4CLgeuAD1bVK4BvApeOc1BJ0ncb9RTKOuD4JOuAE4AngdcBtw7btwGbln48SdKhLBjwqnoC+FPgMWbC/RywA3i2qvYNuz0OnDLf7ZNsSTKVZGp6enppppYkjXQK5STgAuA04EeAFwLnj3oHVbW1qiaranJiYuKIB5UkfbdRTqGcB3ytqqar6tvAbcA5wInDKRWAU4EnxjSjJGkeowT8MeC1SU5IEuBc4GHgHuDCYZ/NwO3jGVGSNJ9RzoFvZ+aXlfcBDw632QpcBbw3yW7gJcCNY5xTkjTHSB8nW1XXANfMWb0HOHvJJ5IkjcQrMSWpKQMuSU0ZcElqyoBLUlMGXJKaMuCS1JQBl6SmDLgkNWXAJakpAy5JTRlwSWrKgEtSUwZckpoy4JLUlAGXpKYMuCQ1ZcAlqSkDLklNGXBJasqAS1JTBlySmjLgktSUAZekpgy4JDVlwCWpKQMuSU0ZcElqyoBLUlMGXJKaMuCS1JQBl6SmDLgkNWXAJakpAy5JTRlwSWrKgEtSUyMFPMmJSW5N8pUku5L8bJIXJ7kzySPD15PGPawk6YBRj8CvB/6+qn4c+GlgF3A1cFdVnQ7cNSxLkpbJggFP8iLgF4EbAarq+ap6FrgA2Dbstg3YNK4hJUkHG+UI/DRgGvhEkvuT3JDkhcCGqnpy2OcpYMO4hpQkHWyUgK8DzgI+WlWvBv6bOadLqqqAmu/GSbYkmUoyNT09vdh5JUmDUQL+OPB4VW0flm9lJuhPJzkZYPj6zHw3rqqtVTVZVZMTExNLMbMkiRECXlVPAV9P8sph1bnAw8AdwOZh3Wbg9rFMKEma17oR97scuDnJscAe4O3MxP/TSS4F9gIXjWdESdJ8Rgp4VT0ATM6z6dylHUeSNCqvxJSkpgy4JDVlwCWpKQMuSU0ZcElqyoBLUlMGXJKaMuCS1JQBl6SmDLgkNWXAJakpAy5JTRlwSWrKgEtSUwZckpoy4JLUlAGXpKYMuCQ1ZcAlqSkDLklNGXBJasqAS1JTBlySmjLgktSUAZekpgy4JDVlwCWpKQMuSU0ZcElqyoBLUlMGXJKaMuCS1JQBl6SmDLgkNWXAJakpAy5JTRlwSWpq5IAnOSbJ/Un+blg+Lcn2JLuTfCrJseMbU5I01/dyBH4FsGvW8nXAB6vqFcA3gUuXcjBJ0uGNFPAkpwJvAG4YlgO8Drh12GUbsGkcA0qS5jfqEfiHgN8H/ndYfgnwbFXtG5YfB06Z74ZJtiSZSjI1PT29qGElSQcsGPAkbwSeqaodR3IHVbW1qiaranJiYuJIfoQkaR7rRtjnHOBNSV4PHAf8EHA9cGKSdcNR+KnAE+MbU5I014JH4FX1/qo6tao2AhcDd1fVbwL3ABcOu20Gbh/blJKkgyzmfeBXAe9NspuZc+I3Ls1IkqRRjHIK5f9V1b3AvcP3e4Czl34kSdIovBJTkpoy4JLUlAGXpKYMuCQ1ZcAlqSkDLklNGXBJasqAS1JTBlySmjLgktSUAZekpgy4JDVlwCWpKQMuSU0ZcElqyoBLUlMGXJKaMuCS1JQBl6SmDLgkNWXAJakpAy5JTRlwSWrKgEtSUwZckpoy4JLUlAGXpKYMuCQ1ZcAlqSkDLklNGXBJasqAS1JTBlySmjLgktSUAZekpgy4JDW1YMCTvDTJPUkeTrIzyRXD+hcnuTPJI8PXk8Y/riRpv1GOwPcB76uqM4DXApclOQO4Grirqk4H7hqWJUnLZMGAV9WTVXXf8P1/AbuAU4ALgG3DbtuATeMaUpJ0sO/pHHiSjcCrge3Ahqp6ctj0FLDhELfZkmQqydT09PQiRpUkzTZywJP8APAZ4N1V9Z+zt1VVATXf7apqa1VNVtXkxMTEooaVJB0wUsCTfD8z8b65qm4bVj+d5ORh+8nAM+MZUZI0n1HehRLgRmBXVf3ZrE13AJuH7zcDty/9eJKkQ1k3wj7nAJcADyZ5YFj3B8C1wKeTXArsBS4az4iSpPksGPCq+iKQQ2w+d2nHkSSNyisxJakpAy5JTRlwSWrKgEtSUwZckpoy4JLUlAGXpKYMuCQ1ZcAlqSkDLklNGXBJasqAS1JTBlySmjLgktSUAZekpgy4JDVlwCWpKQMuSU0ZcElqyoBLUlMGXJKaMuCS1JQBl6SmDLgkNWXAJakpAy5JTRlwSWrKgEtSUwZckpoy4JLUlAGXpKYMuCQ1ZcAlqSkDLklNGXBJasqAS1JTBlySmlpUwJOcn+SrSXYnuXqphpIkLeyIA57kGOAjwK8BZwBvTXLGUg0mSTq8xRyBnw3srqo9VfU8cAtwwdKMJUlaSKrqyG6YXAicX1XvGJYvAX6mqt45Z78twJZh8ZXAV4983BW1HvjGSg/RmI/f4vj4LU73x+9lVTUxd+W6cd9rVW0Fto77fsYtyVRVTa70HF35+C2Oj9/irNXHbzGnUJ4AXjpr+dRhnSRpGSwm4P8KnJ7ktCTHAhcDdyzNWJKkhRzxKZSq2pfkncA/AMcAH6+qnUs22erT/jTQCvPxWxwfv8VZk4/fEf8SU5K0srwSU5KaMuCS1JQBB5L8cJJbkvx7kh1JvpDkx5JUkstn7ffnSd42fH9TkieSvGBYXp/k0ZX5GyyvJN9J8sCsPxtXeqa1JMmGJH+VZM/wfPxSkjcn+eUkz8157M9b6XlX0qzn4kNJ/jbJicP6jUfD6/eoD3iSAJ8F7q2ql1fVa4D3AxuAZ4ArhnfZzOc7wG8vz6Sryreq6lWz/jy60gOtFcPz8XPAP1XVjw7Px4uZeZsuwD/Peez/ccWGXR32PxfPBP4DuGzWtjX/+j3qAw78CvDtqvrY/hVV9W/A14Fp4C5g8yFu+yHgPUnGfkHUapfk0STrh+8nk9w7fP+FWUeLzyXZnOSGWeumk1yT5JNJNs36eTcnORo/muF1wPNzno97q+rDKzhTF18CTpm1vOZfvwYczgR2HGb7dcCVw4d3zfUY8EXgknEMtoodPyvAnz3cjlX1+qp6FXApsBf4XFW9Y1h3ATOXN98E3Ai8DSDJi4CfAz4/vr/CqvUTwH2H2f4Lc06hvHy5BlvNhtfnuRx8Lcqafv22/ZdnuVTVniTbgd84xC5/AtzO0RWbbw0BHslwZP6XwEVV9dyw7jjgb4DLq2ovsDfJXySZAN4CfKaq9o1h9laSfAT4eeB54PeYOYXyxpWdalU5PskDzBx57wLunL1xrb9+PQKHncBrFtjnj4GrgMzdUFWPAA8AFy39aK3s48Dz6bj9K4cjn1uAP6qqh2bt/zHgtjnncD8J/BbwduDj4x131doJnLV/oaouY+bI8qAPMhJw4GDiZcy8Pi+bZ581+/o14HA38ILhUxMBSPJTzPqcl6r6CvAw8OuH+BkfAK4c55ANPMqBfwjfMmv9tcCXq+qW/SuSXAb8YFVdO+dn3AS8G6CqHh7bpKvb3cBxSX531roTVmqYLqrqf4B3Ae+be057Lb9+j/qA18ylqG8GzhveRriTmf9WPTVn1w9w4J0Ac3/GTg5/3vJo8IfA9UmmmPnt/n5XAr8665ztm4Z1Pzlr3e8AVNXTzPw3+BPLPfxqMTwfNwG/lORrSf4F2MbMESQcfA78whUbdpWpqvuBLwNvnWfzmnz9eim9Vo0kJwAPAmftP1cu6dCO+iNwrQ7DBSm7gA8bb2k0HoFLUlMegUtSUwZckpoy4JLUlAGXpKYMuCQ19X/vNPyNaL2SXwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn5TEfuhPAC-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "outputId": "c437ba38-cee6-4053-c2ee-7f3862c060dc"
      },
      "source": [
        "# Passing the parameters to the bar function, this is the main function which creates the bar plot\n",
        "plt.figure(figsize=(12,7))\n",
        "Model=result_df['Model'].tolist()\n",
        "Score=result_df['Score'].tolist()\n",
        "plt.bar(Model, Score, width= 0.9, align='center',color='cyan', edgecolor = 'red')\n",
        "# This is the location for the annotated text\n",
        "i = 4.0\n",
        "j = 1\n",
        "# Annotating the bar plot with the values (total death count)\n",
        "for i in range(len(Model)):\n",
        "    plt.annotate(Score[i], (-0.1 + i, Score[i] + j))\n",
        "# Creating the legend of the bars in the plot\n",
        "plt.legend(labels = ['Score'])\n",
        "# Giving the tilte for the plot\n",
        "plt.title(\"Bar plot representing Breast Cancer Models Prediction\")\n",
        "# Namimg the x and y axis\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Score')\n",
        "# Saving the plot as a 'png'\n",
        "#plt.savefig('1BarPlot.png')\n",
        "# Displaying the bar plot\n",
        "#plt.show()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Score')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}